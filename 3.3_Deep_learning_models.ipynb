{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "plt.rc('font',family='Times New Roman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个模型配置类\n",
    "class Config:\n",
    "    def __init__(self, data_path='./Data/PRSA_Data/Data_Aotizhongxin.csv', timestep=1, train_size=0.75, model_name='model'):\n",
    "        self.data_path = data_path\n",
    "        self.timestep = timestep\n",
    "        self.train_size = train_size\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'data_path':\n",
    "            self.data_path = value\n",
    "        elif param == 'timestep':\n",
    "            self.timestep = value\n",
    "        elif param == 'train_size':\n",
    "            self.train_size = value\n",
    "        elif param == 'model_name':\n",
    "            self.model_name = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据并标准化（归一化）\n",
    "# 数据标准化（归一化）\n",
    "# 标准化：对原始数据进行变换把数据变换到均值为0,方差为1范围内\n",
    "# 归一化：对每个特征缩放到给定范围(默认[0,1])\n",
    "# 一般只对自变量X进行标准化（归一化）\n",
    "def read_data(data_path):\n",
    "    df_data = pd.read_csv(data_path, index_col=0)\n",
    "    # 采用IQR异常值检验和线性插值\n",
    "    df = df_data.copy()\n",
    "    df = IQR(df_data, df)\n",
    "    df = df.interpolate()\n",
    "    scaler = StandardScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(np.array(df)))\n",
    "    df.reset_index(inplace=True)\n",
    "    data = pd.concat([data.iloc[:,1:], df.iloc[:,1]], axis=1,join='outer')\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "def IQR(data, new_data):\n",
    "    for i in range(len(np.array(data.columns))):\n",
    "        df_25 = data.iloc[:,i].quantile(0.25)\n",
    "        df_75 = data.iloc[:,i].quantile(0.75)\n",
    "        IQR = df_75 - df_25\n",
    "        lower_limit = df_25 - 1.5*IQR\n",
    "        upper_limit = df_75 + 1.5*IQR\n",
    "        new_data.iloc[:,i] = data.iloc[:,i][(data.iloc[:,i]>lower_limit) & (data.iloc[:,i]<upper_limit)]\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集、测试集\n",
    "def split_data(data, timestep, train_size):\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    \n",
    "    # 将整个窗口的数据保存到X中，将未来timestep保存到Y中\n",
    "    for index in range(len(data)- timestep):\n",
    "        data_X.append(data[index: index + timestep])\n",
    "        data_Y.append(data[index + timestep][-1])\n",
    "    \n",
    "    dataX = np.array(data_X)\n",
    "    dataY = np.array(data_Y)\n",
    "    \n",
    "    train_size = int(np.round(train_size * dataX.shape[0]))\n",
    "    \n",
    "    x_train = dataX[: train_size, :].reshape(-1, timestep, data.shape[1])\n",
    "    y_train = dataY[: train_size].reshape(-1, 1)\n",
    "    \n",
    "    x_test = dataX[train_size:, :].reshape(-1, timestep, data.shape[1])\n",
    "    y_test = dataY[train_size:].reshape(-1, 1)\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], (x_train.shape[1] * x_train.shape[2]))\n",
    "    x_test = x_test.reshape(x_test.shape[0], (x_test.shape[1] * x_test.shape[2]))\n",
    "    \n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    x_test = pd.DataFrame(x_test)\n",
    "        \n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_cnn(data, timestep, train_size):\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    \n",
    "    for index in range(len(data) - timestep):\n",
    "        data_X.append(data[index: index + timestep])\n",
    "        data_Y.append(data[index + timestep][-1])\n",
    "        \n",
    "    dataX = np.array(data_X)\n",
    "    dataY = np.array(data_Y)\n",
    "    \n",
    "    train_size = int(np.round(train_size * dataX.shape[0]))\n",
    "    x_train = dataX[: train_size, :].reshape(-1, timestep, data.shape[1])\n",
    "    y_train = dataY[: train_size]\n",
    "    \n",
    "    x_test = dataX[train_size:, :].reshape(-1, timestep, data.shape[1])\n",
    "    y_test = dataY[train_size: ]\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转为tensor，并加载为迭代器\n",
    "def tensor_load_cnn(x_train, y_train, x_test, y_test, batch_size):\n",
    "    x_train_tensor = torch.from_numpy(x_train).to(torch.float32)\n",
    "    y_train_tensor = torch.from_numpy(y_train).to(torch.float32)\n",
    "    x_test_tensor = torch.from_numpy(x_test).to(torch.float32)\n",
    "    y_test_tensor = torch.from_numpy(y_test).to(torch.float32)\n",
    "    \n",
    "    train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size, False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size, False)\n",
    "    return x_test_tensor, y_test_tensor, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转为tensor，并加载为迭代器\n",
    "def tensor_load(x_train, y_train, x_test, y_test, batch_size):\n",
    "    x_train_tensor = torch.from_numpy(x_train.values).to(torch.float32)\n",
    "    y_train_tensor = torch.from_numpy(y_train).to(torch.float32)\n",
    "    x_test_tensor = torch.from_numpy(x_test.values).to(torch.float32)\n",
    "    y_test_tensor = torch.from_numpy(y_test).to(torch.float32)\n",
    "    \n",
    "    train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size, False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size, False)\n",
    "    return x_test_tensor, y_test_tensor, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用各种机器学习算法进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用训练并保存好的模型进行预测\n",
    "def prediction(x_test_tensor, y_test_tensor, config):\n",
    "    save_path = './Model/{}.pth'.format(config.model_name)\n",
    "    model = torch.load(save_path)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    y_pre = model(x_test_tensor)\n",
    "    y_pre = y_pre.to(torch.device('cpu')).detach().numpy()\n",
    "    NSE =r2_score(y_test_tensor,y_pre)\n",
    "    MAE =mean_absolute_error(y_test_tensor,y_pre)\n",
    "    return y_pre, NSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存预测结果\n",
    "def save_results(y_test_tensor, y_pre, config):\n",
    "    file_path = './Results/{}.csv'.format(config.model_name)\n",
    "    df = pd.read_csv(config.data_path, index_col=0)\n",
    "    train_size = int(np.round(config.train_size * df.shape[0]))\n",
    "    index = df.index.values[train_size:]\n",
    "    index = pd.to_datetime(pd.DataFrame(index).iloc[:,0]).values\n",
    "    y_test_tensor = torch.squeeze(y_test_tensor).tolist()\n",
    "    y_pre = y_pre.ravel()\n",
    "    result = pd.DataFrame({'datetime': index, 'Measured values': y_test_tensor, 'Predicted values':y_pre})\n",
    "    result.to_csv(file_path, index=False, sep=',')\n",
    "    \n",
    "def save_results_cnn(y_test_tensor, y_pre, config):\n",
    "    file_path = './Results/{}.csv'.format(config.model_name)\n",
    "    y_test_tensor = torch.squeeze(y_test_tensor).tolist()\n",
    "    y_pre = y_pre.ravel()\n",
    "    result = pd.DataFrame({'Measured values': y_test_tensor, 'Predicted values':y_pre})\n",
    "    result.to_csv(file_path, index=False, sep=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. MLPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建网络超参数类\n",
    "class NN_config:\n",
    "    def __init__(self, batch_size=8, feature_size=11, hidden_size1=100, hidden_size2=50, output_size=1, epochs=50, learning_rate=1e-8, best_loss=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_loss = best_loss\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'batch_size':\n",
    "            self.batch_size = value\n",
    "        elif param == 'feature_size':\n",
    "            self.feature_size = value\n",
    "        elif param == 'hidden_size1':\n",
    "            self.hidden_size1 = value\n",
    "        elif param == 'hidden_size_2':\n",
    "            self.hidden_size_2 = value\n",
    "        elif param == 'output_size':\n",
    "            self.output_size = value\n",
    "        elif param == 'epochs':\n",
    "            self.epochs = value\n",
    "        elif param == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif param == 'best_loss':\n",
    "            self.best_loss = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')\n",
    "\n",
    "# 定义MLP网络\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(in_features=feature_size, out_features=hidden_size1, bias=True)\n",
    "        self.hidden2 = nn.Linear(hidden_size1, hidden_size1)\n",
    "        self.hidden3 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.predict = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        x.to(device)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = self.predict(x)\n",
    "        return x[:, 0]\n",
    "    \n",
    "# 模型训练并保存\n",
    "def MLPNN(train_loader, test_loader, config, nn_config):\n",
    "    # 选择训练硬件设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义MLP网络\n",
    "    model = MLP(nn_config.feature_size, nn_config.hidden_size1, nn_config.hidden_size2, nn_config.output_size).to(device)\n",
    "    # 定义损失函数\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=nn_config.learning_rate)\n",
    "    # 模型训练\n",
    "    train_loss_plot = []\n",
    "    test_loss_plot = []\n",
    "    for epoch in range(nn_config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        loss_plot = []\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_train_pred = model(x_train)\n",
    "            loss = loss_function(y_train_pred, y_train.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_plot.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, nn_config.epochs, loss)\n",
    "        train_loss_plot.append(min(loss_plot))\n",
    "            \n",
    "    # 模型验证\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = loss_function(y_test_pred, y_test.reshape(-1, 1))\n",
    "            test_loss_plot.append(test_loss.item())\n",
    "    \n",
    "    save_path = './Model/{}.pth'.format(config.model_name)        \n",
    "    if test_loss < nn_config.best_loss:\n",
    "        nn_config.modify_params('best_loss', test_loss)\n",
    "        torch.save(model, save_path)\n",
    "    else:\n",
    "        torch.save(model, save_path)\n",
    "    return train_loss_plot, test_loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/50] loss:189.634: 100%|██████████| 3507/3507 [00:04<00:00, 869.62it/s]  \n",
      "train epoch[2/50] loss:176.138: 100%|██████████| 3507/3507 [00:03<00:00, 930.68it/s]   \n",
      "train epoch[3/50] loss:137.213: 100%|██████████| 3507/3507 [00:03<00:00, 933.29it/s]   \n",
      "train epoch[4/50] loss:67.441: 100%|██████████| 3507/3507 [00:03<00:00, 896.42it/s]   \n",
      "train epoch[5/50] loss:55.510: 100%|██████████| 3507/3507 [00:04<00:00, 805.35it/s]  \n",
      "train epoch[6/50] loss:55.295: 100%|██████████| 3507/3507 [00:04<00:00, 853.66it/s]  \n",
      "train epoch[7/50] loss:55.274: 100%|██████████| 3507/3507 [00:03<00:00, 950.00it/s]   \n",
      "train epoch[8/50] loss:55.259: 100%|██████████| 3507/3507 [00:03<00:00, 904.37it/s]  \n",
      "train epoch[9/50] loss:55.247: 100%|██████████| 3507/3507 [00:03<00:00, 900.82it/s]  \n",
      "train epoch[10/50] loss:55.234: 100%|██████████| 3507/3507 [00:03<00:00, 911.82it/s]  \n",
      "train epoch[11/50] loss:55.220: 100%|██████████| 3507/3507 [00:03<00:00, 917.54it/s]  \n",
      "train epoch[12/50] loss:55.206: 100%|██████████| 3507/3507 [00:04<00:00, 834.82it/s]  \n",
      "train epoch[13/50] loss:55.192: 100%|██████████| 3507/3507 [00:04<00:00, 820.67it/s]   \n",
      "train epoch[14/50] loss:55.178: 100%|██████████| 3507/3507 [00:04<00:00, 843.12it/s]  \n",
      "train epoch[15/50] loss:55.163: 100%|██████████| 3507/3507 [00:03<00:00, 894.90it/s]   \n",
      "train epoch[16/50] loss:55.148: 100%|██████████| 3507/3507 [00:03<00:00, 945.95it/s]  \n",
      "train epoch[17/50] loss:55.132: 100%|██████████| 3507/3507 [00:03<00:00, 949.75it/s]  \n",
      "train epoch[18/50] loss:55.117: 100%|██████████| 3507/3507 [00:03<00:00, 955.73it/s]  \n",
      "train epoch[19/50] loss:55.101: 100%|██████████| 3507/3507 [00:04<00:00, 826.80it/s]   \n",
      "train epoch[20/50] loss:55.086: 100%|██████████| 3507/3507 [00:04<00:00, 788.53it/s]  \n",
      "train epoch[21/50] loss:55.070: 100%|██████████| 3507/3507 [00:03<00:00, 931.32it/s]  \n",
      "train epoch[22/50] loss:55.055: 100%|██████████| 3507/3507 [00:03<00:00, 927.18it/s]  \n",
      "train epoch[23/50] loss:55.040: 100%|██████████| 3507/3507 [00:03<00:00, 892.16it/s]  \n",
      "train epoch[24/50] loss:55.024: 100%|██████████| 3507/3507 [00:03<00:00, 917.26it/s]  \n",
      "train epoch[25/50] loss:55.009: 100%|██████████| 3507/3507 [00:03<00:00, 921.68it/s]   \n",
      "train epoch[26/50] loss:54.994: 100%|██████████| 3507/3507 [00:04<00:00, 875.53it/s]  \n",
      "train epoch[27/50] loss:54.979: 100%|██████████| 3507/3507 [00:04<00:00, 845.73it/s]  \n",
      "train epoch[28/50] loss:54.963: 100%|██████████| 3507/3507 [00:03<00:00, 914.93it/s]   \n",
      "train epoch[29/50] loss:54.948: 100%|██████████| 3507/3507 [00:03<00:00, 925.08it/s]  \n",
      "train epoch[30/50] loss:54.933: 100%|██████████| 3507/3507 [00:03<00:00, 889.74it/s]  \n",
      "train epoch[31/50] loss:54.918: 100%|██████████| 3507/3507 [00:03<00:00, 900.45it/s]  \n",
      "train epoch[32/50] loss:54.903: 100%|██████████| 3507/3507 [00:03<00:00, 913.25it/s]  \n",
      "train epoch[33/50] loss:54.888: 100%|██████████| 3507/3507 [00:04<00:00, 857.32it/s]  \n",
      "train epoch[34/50] loss:54.873: 100%|██████████| 3507/3507 [00:04<00:00, 867.25it/s]  \n",
      "train epoch[35/50] loss:54.858: 100%|██████████| 3507/3507 [00:03<00:00, 889.02it/s]  \n",
      "train epoch[36/50] loss:54.843: 100%|██████████| 3507/3507 [00:03<00:00, 915.36it/s]   \n",
      "train epoch[37/50] loss:54.828: 100%|██████████| 3507/3507 [00:03<00:00, 890.02it/s]  \n",
      "train epoch[38/50] loss:54.813: 100%|██████████| 3507/3507 [00:03<00:00, 1000.49it/s]  \n",
      "train epoch[39/50] loss:54.798: 100%|██████████| 3507/3507 [00:03<00:00, 950.80it/s]  \n",
      "train epoch[40/50] loss:54.783: 100%|██████████| 3507/3507 [00:03<00:00, 966.97it/s]  \n",
      "train epoch[41/50] loss:54.768: 100%|██████████| 3507/3507 [00:04<00:00, 825.49it/s]   \n",
      "train epoch[42/50] loss:54.753: 100%|██████████| 3507/3507 [00:04<00:00, 849.75it/s]  \n",
      "train epoch[43/50] loss:54.738: 100%|██████████| 3507/3507 [00:04<00:00, 842.60it/s]   \n",
      "train epoch[44/50] loss:54.723: 100%|██████████| 3507/3507 [00:03<00:00, 919.95it/s]  \n",
      "train epoch[45/50] loss:54.709: 100%|██████████| 3507/3507 [00:03<00:00, 921.60it/s]   \n",
      "train epoch[46/50] loss:54.694: 100%|██████████| 3507/3507 [00:03<00:00, 882.55it/s]  \n",
      "train epoch[47/50] loss:54.679: 100%|██████████| 3507/3507 [00:04<00:00, 859.12it/s]  \n",
      "train epoch[48/50] loss:54.664: 100%|██████████| 3507/3507 [00:04<00:00, 862.33it/s]  \n",
      "train epoch[49/50] loss:54.650: 100%|██████████| 3507/3507 [00:03<00:00, 881.16it/s]   \n",
      "train epoch[50/50] loss:54.635: 100%|██████████| 3507/3507 [00:03<00:00, 897.28it/s]   \n",
      "100%|██████████| 877/877 [00:00<00:00, 2560.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE= 0.9411589341083014\n",
      "MAE= 9.7638035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型调用过程\n",
    "# 1.初始化配置\n",
    "config = Config(train_size=0.8)\n",
    "# 2.初始化模型超参数\n",
    "nn_config = NN_config()\n",
    "# 3.根据使用模型修改名称\n",
    "config.modify_params('model_name', 'MLP')\n",
    "# 4.读取数据\n",
    "data = read_data(config.data_path)\n",
    "# 5.划分训练集、测试集\n",
    "x_train, y_train, x_test, y_test = split_data(data, config.timestep, config.train_size)\n",
    "# 6.将数据格式转为tensor\n",
    "x_test_tensor, y_test_tensor, train_loader, test_loader = tensor_load(x_train, y_train, x_test, y_test, nn_config.batch_size)\n",
    "# 7.模型训练\n",
    "train_loss_plot, test_loss_plot = MLPNN(train_loader, test_loader, config, nn_config)\n",
    "# 8.模型预测\n",
    "y_pre, NSE, MAE = prediction(x_test_tensor, y_test_tensor, config)\n",
    "# 9.结果保存\n",
    "save_results(y_test_tensor, y_pre, config)\n",
    "# 10.绘图展示（4.中展示）\n",
    "print(\"NSE=\", NSE)\n",
    "print(\"MAE=\", MAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.循环神经网络（RNN）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 传统RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建网络超参数类\n",
    "class NN_config:\n",
    "    def __init__(self, batch_size=8, feature_size=11, hidden_size=128, num_layers=2, output_size=1, epochs=50, learning_rate=1e-8, best_loss=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_loss = best_loss\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'batch_size':\n",
    "            self.batch_size = value\n",
    "        elif param == 'feature_size':\n",
    "            self.feature_size = value\n",
    "        elif param == 'hidden_size':\n",
    "            self.hidden_size = value\n",
    "        elif param == 'num_layers':\n",
    "            self.num_layers = value\n",
    "        elif param == 'output_size':\n",
    "            self.output_size = value\n",
    "        elif param == 'epochs':\n",
    "            self.epochs = value\n",
    "        elif param == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif param == 'best_loss':\n",
    "            self.best_loss = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')\n",
    "        \n",
    "# 定义RNN网络\n",
    "# 训练过程中可能出现预测结果为一条直线的情况，解决办法请参照：https://blog.csdn.net/m0_47256162/article/details/128720691#:~:text=%E5%AF%B9%E4%BA%8E%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE,%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E3%80%82\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # feature_size为特征维度，就是每个时间点对应的特征数量\n",
    "        self.rnn = nn.RNN(feature_size, hidden_size, num_layers, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.shape[0]  # 获取批次大小\n",
    "        if hidden is None:\n",
    "            h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n",
    "        else:\n",
    "            h_0 = hidden\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        output, h_0 = self.rnn(x, h_0)\n",
    "        output = self.fc(output)  # 形状为batch_size * timestep, 1\n",
    "        return output[:, -1, :]  # 只需要返回最后一个时间切片的数据\n",
    "    \n",
    "# 模型训练并保存\n",
    "def RNN_model(train_loader, test_loader, config, nn_config):\n",
    "    # 选择训练硬件设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义RNN网络\n",
    "    model = RNN(nn_config.feature_size, nn_config.hidden_size, nn_config.num_layers, nn_config.output_size).to(device)\n",
    "    # 定义损失函数\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nn_config.learning_rate)\n",
    "    # 模型训练\n",
    "    train_loss_plot = []\n",
    "    test_loss_plot = []\n",
    "    for epoch in range(nn_config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        loss_plot = []\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_train_pred = model(x_train)\n",
    "            loss = loss_function(y_train_pred, y_train.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_plot.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, nn_config.epochs, loss)\n",
    "        train_loss_plot.append(min(loss_plot))\n",
    "            \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = loss_function(y_test_pred, y_test.reshape(-1, 1))\n",
    "            test_loss_plot.append(test_loss)\n",
    "    \n",
    "    save_path = './Model/{}.pth'.format(config.model_name)\n",
    "    if test_loss < nn_config.best_loss:\n",
    "        nn_config.modify('best_loss', test_loss)\n",
    "        torch.save(model, save_path)\n",
    "    else:\n",
    "        torch.save(model, save_path)\n",
    "    return train_loss_plot, test_loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/60] loss:186.006: 100%|██████████| 3507/3507 [00:04<00:00, 703.58it/s]  \n",
      "train epoch[2/60] loss:180.169: 100%|██████████| 3507/3507 [00:04<00:00, 774.77it/s]  \n",
      "train epoch[3/60] loss:174.413: 100%|██████████| 3507/3507 [00:04<00:00, 725.05it/s]  \n",
      "train epoch[4/60] loss:168.553: 100%|██████████| 3507/3507 [00:04<00:00, 717.67it/s]  \n",
      "train epoch[5/60] loss:162.527: 100%|██████████| 3507/3507 [00:04<00:00, 709.41it/s]  \n",
      "train epoch[6/60] loss:156.224: 100%|██████████| 3507/3507 [00:05<00:00, 654.03it/s]  \n",
      "train epoch[7/60] loss:149.464: 100%|██████████| 3507/3507 [00:07<00:00, 472.23it/s]  \n",
      "train epoch[8/60] loss:142.515: 100%|██████████| 3507/3507 [00:09<00:00, 363.00it/s]  \n",
      "train epoch[9/60] loss:135.277: 100%|██████████| 3507/3507 [00:09<00:00, 363.11it/s]  \n",
      "train epoch[10/60] loss:128.030: 100%|██████████| 3507/3507 [00:09<00:00, 369.07it/s]  \n",
      "train epoch[11/60] loss:120.716: 100%|██████████| 3507/3507 [00:06<00:00, 511.43it/s]  \n",
      "train epoch[12/60] loss:113.636: 100%|██████████| 3507/3507 [00:10<00:00, 347.94it/s]  \n",
      "train epoch[13/60] loss:106.965: 100%|██████████| 3507/3507 [00:10<00:00, 327.86it/s]  \n",
      "train epoch[14/60] loss:100.952: 100%|██████████| 3507/3507 [00:10<00:00, 323.77it/s]  \n",
      "train epoch[15/60] loss:95.813: 100%|██████████| 3507/3507 [00:10<00:00, 325.31it/s]   \n",
      "train epoch[16/60] loss:91.764: 100%|██████████| 3507/3507 [00:10<00:00, 321.55it/s]  \n",
      "train epoch[17/60] loss:89.113: 100%|██████████| 3507/3507 [00:06<00:00, 556.78it/s]  \n",
      "train epoch[18/60] loss:88.080: 100%|██████████| 3507/3507 [00:06<00:00, 579.87it/s]  \n",
      "train epoch[19/60] loss:88.787: 100%|██████████| 3507/3507 [00:06<00:00, 577.68it/s]  \n",
      "train epoch[20/60] loss:90.842: 100%|██████████| 3507/3507 [00:05<00:00, 599.16it/s]  \n",
      "train epoch[21/60] loss:92.737: 100%|██████████| 3507/3507 [00:05<00:00, 596.24it/s] \n",
      "train epoch[22/60] loss:93.334: 100%|██████████| 3507/3507 [00:05<00:00, 684.23it/s]  \n",
      "train epoch[23/60] loss:93.440: 100%|██████████| 3507/3507 [00:05<00:00, 661.48it/s] \n",
      "train epoch[24/60] loss:93.479: 100%|██████████| 3507/3507 [00:05<00:00, 586.43it/s]  \n",
      "train epoch[25/60] loss:93.497: 100%|██████████| 3507/3507 [00:06<00:00, 562.86it/s]  \n",
      "train epoch[26/60] loss:93.509: 100%|██████████| 3507/3507 [00:05<00:00, 634.75it/s]  \n",
      "train epoch[27/60] loss:93.518: 100%|██████████| 3507/3507 [00:05<00:00, 657.86it/s]  \n",
      "train epoch[28/60] loss:93.524: 100%|██████████| 3507/3507 [00:04<00:00, 708.80it/s]  \n",
      "train epoch[29/60] loss:93.527: 100%|██████████| 3507/3507 [00:06<00:00, 576.06it/s]  \n",
      "train epoch[30/60] loss:93.528: 100%|██████████| 3507/3507 [00:10<00:00, 326.27it/s]  \n",
      "train epoch[31/60] loss:93.527: 100%|██████████| 3507/3507 [00:11<00:00, 312.78it/s]  \n",
      "train epoch[32/60] loss:93.526: 100%|██████████| 3507/3507 [00:10<00:00, 326.67it/s]  \n",
      "train epoch[33/60] loss:93.521: 100%|██████████| 3507/3507 [00:07<00:00, 478.32it/s]  \n",
      "train epoch[34/60] loss:93.512: 100%|██████████| 3507/3507 [00:05<00:00, 643.61it/s]  \n",
      "train epoch[35/60] loss:93.500: 100%|██████████| 3507/3507 [00:05<00:00, 688.00it/s]  \n",
      "train epoch[36/60] loss:93.485: 100%|██████████| 3507/3507 [00:04<00:00, 710.45it/s] \n",
      "train epoch[37/60] loss:93.465: 100%|██████████| 3507/3507 [00:04<00:00, 709.94it/s]  \n",
      "train epoch[38/60] loss:93.441: 100%|██████████| 3507/3507 [00:04<00:00, 706.23it/s]  \n",
      "train epoch[39/60] loss:93.414: 100%|██████████| 3507/3507 [00:05<00:00, 685.64it/s]  \n",
      "train epoch[40/60] loss:93.383: 100%|██████████| 3507/3507 [00:05<00:00, 699.91it/s] \n",
      "train epoch[41/60] loss:93.349: 100%|██████████| 3507/3507 [00:04<00:00, 762.00it/s]  \n",
      "train epoch[42/60] loss:93.310: 100%|██████████| 3507/3507 [00:04<00:00, 705.19it/s]  \n",
      "train epoch[43/60] loss:93.266: 100%|██████████| 3507/3507 [00:05<00:00, 661.56it/s]  \n",
      "train epoch[44/60] loss:93.220: 100%|██████████| 3507/3507 [00:05<00:00, 647.24it/s]  \n",
      "train epoch[45/60] loss:93.169: 100%|██████████| 3507/3507 [00:05<00:00, 655.57it/s]  \n",
      "train epoch[46/60] loss:93.116: 100%|██████████| 3507/3507 [00:05<00:00, 674.02it/s]  \n",
      "train epoch[47/60] loss:93.059: 100%|██████████| 3507/3507 [00:05<00:00, 683.92it/s]  \n",
      "train epoch[48/60] loss:92.988: 100%|██████████| 3507/3507 [00:04<00:00, 717.97it/s] \n",
      "train epoch[49/60] loss:92.911: 100%|██████████| 3507/3507 [00:04<00:00, 782.64it/s] \n",
      "train epoch[50/60] loss:92.832: 100%|██████████| 3507/3507 [00:05<00:00, 695.26it/s] \n",
      "train epoch[51/60] loss:92.748: 100%|██████████| 3507/3507 [00:05<00:00, 615.95it/s]  \n",
      "train epoch[52/60] loss:92.662: 100%|██████████| 3507/3507 [00:05<00:00, 686.78it/s]  \n",
      "train epoch[53/60] loss:92.573: 100%|██████████| 3507/3507 [00:05<00:00, 599.10it/s]  \n",
      "train epoch[54/60] loss:92.482: 100%|██████████| 3507/3507 [00:09<00:00, 353.02it/s]  \n",
      "train epoch[55/60] loss:92.391: 100%|██████████| 3507/3507 [00:11<00:00, 304.42it/s]  \n",
      "train epoch[56/60] loss:92.298: 100%|██████████| 3507/3507 [00:11<00:00, 303.44it/s]  \n",
      "train epoch[57/60] loss:92.210: 100%|██████████| 3507/3507 [00:11<00:00, 311.88it/s]  \n",
      "train epoch[58/60] loss:92.123: 100%|██████████| 3507/3507 [00:07<00:00, 499.17it/s]  \n",
      "train epoch[59/60] loss:92.034: 100%|██████████| 3507/3507 [00:05<00:00, 656.39it/s] \n",
      "train epoch[60/60] loss:91.942: 100%|██████████| 3507/3507 [00:05<00:00, 660.27it/s]  \n",
      "100%|██████████| 877/877 [00:00<00:00, 2636.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE= 0.9449101191576368\n",
      "MAE= 8.836823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型调用过程\n",
    "# 1.初始化配置\n",
    "config = Config(train_size=0.8)\n",
    "# 2.初始化模型超参数\n",
    "nn_config = NN_config(learning_rate=1e-6, epochs=60)\n",
    "# 3.根据使用模型修改名称\n",
    "config.modify_params('model_name', 'rnn')\n",
    "# 4.读取数据\n",
    "data = read_data(config.data_path)\n",
    "# 5.划分训练集、测试集\n",
    "x_train, y_train, x_test, y_test = split_data(data, config.timestep, config.train_size)\n",
    "# 6.将数据格式转为tensor\n",
    "x_test_tensor, y_test_tensor, train_loader, test_loader = tensor_load(x_train, y_train, x_test, y_test, nn_config.batch_size)\n",
    "# 7.模型训练\n",
    "train_loss_plot, test_loss_plot = RNN_model(train_loader, test_loader, config, nn_config)\n",
    "# 8.模型预测\n",
    "y_pre, NSE, MAE = prediction(x_test_tensor, y_test_tensor, config)\n",
    "# 9.结果保存\n",
    "save_results(y_test_tensor, y_pre, config)\n",
    "# 10.绘图展示（4.中展示）\n",
    "print(\"NSE=\", NSE)\n",
    "print(\"MAE=\", MAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建网络超参数类\n",
    "class NN_config:\n",
    "    def __init__(self, batch_size=16, feature_size=11, hidden_size=128, num_layers=2, output_size=1, epochs=50, learning_rate=1e-8, best_loss=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_loss = best_loss\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'batch_size':\n",
    "            self.batch_size = value\n",
    "        elif param == 'feature_size':\n",
    "            self.feature_size = value\n",
    "        elif param == 'hidden_size':\n",
    "            self.hidden_size = value\n",
    "        elif param == 'num_layers':\n",
    "            self.num_layers = value\n",
    "        elif param == 'output_size':\n",
    "            self.output_size = value\n",
    "        elif param == 'epochs':\n",
    "            self.epochs = value\n",
    "        elif param == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif param == 'best_loss':\n",
    "            self.best_loss = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')\n",
    "        \n",
    "# 定义LSTM网络\n",
    "# 若使用BiLSTM，需设置nn.LSTM中的超参数bidirectional=True\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # feature_size为特征维度，就是每个时间点对应的特征数量\n",
    "        self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.shape[0]  # 获取批次大小\n",
    "        if hidden is None:\n",
    "            h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n",
    "            c_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        output, (h_0, c_0) = self.lstm(x, (h_0, c_0))\n",
    "        output = self.dropout(output)\n",
    "        output = F.sigmoid(self.fc(output))  # 形状为batch_size * timestep, 1\n",
    "        return output[:, -1, :]  # 只需要返回最后一个时间切片的数据\n",
    "    \n",
    "# 模型训练并保存\n",
    "def LSTM_model(train_loader, test_loader, config, nn_config):\n",
    "    # 选择训练硬件设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义RNN网络\n",
    "    model = LSTM(nn_config.feature_size, nn_config.hidden_size, nn_config.num_layers, nn_config.output_size).to(device)\n",
    "    # 定义损失函数\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nn_config.learning_rate)\n",
    "    # 模型训练\n",
    "    train_loss_plot = []\n",
    "    test_loss_plot = []\n",
    "    for epoch in range(nn_config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        loss_plot = []\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_train_pred = model(x_train)\n",
    "            loss = loss_function(y_train_pred, y_train.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_plot.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, nn_config.epochs, loss)\n",
    "        train_loss_plot.append(min(loss_plot))\n",
    "            \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = loss_function(y_test_pred, y_test.reshape(-1, 1))\n",
    "            test_loss_plot.append(test_loss)\n",
    "    \n",
    "    save_path = './Model/{}.pth'.format(config.model_name)\n",
    "    if test_loss < nn_config.best_loss:\n",
    "        nn_config.modify('best_loss', test_loss)\n",
    "        torch.save(model, save_path)\n",
    "    else:\n",
    "        torch.save(model, save_path)\n",
    "    return train_loss_plot, test_loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/60] loss:168.501: 100%|██████████| 1754/1754 [00:03<00:00, 535.36it/s]  \n",
      "train epoch[2/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 595.57it/s]  \n",
      "train epoch[3/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 613.49it/s]  \n",
      "train epoch[4/60] loss:168.500: 100%|██████████| 1754/1754 [00:03<00:00, 578.16it/s]  \n",
      "train epoch[5/60] loss:168.500: 100%|██████████| 1754/1754 [00:03<00:00, 567.21it/s]  \n",
      "train epoch[6/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 592.95it/s]  \n",
      "train epoch[7/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 590.57it/s]  \n",
      "train epoch[8/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 621.33it/s]  \n",
      "train epoch[9/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 588.99it/s]  \n",
      "train epoch[10/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 591.97it/s]  \n",
      "train epoch[11/60] loss:168.500: 100%|██████████| 1754/1754 [00:03<00:00, 568.19it/s]  \n",
      "train epoch[12/60] loss:168.500: 100%|██████████| 1754/1754 [00:03<00:00, 579.33it/s]  \n",
      "train epoch[13/60] loss:168.500: 100%|██████████| 1754/1754 [00:03<00:00, 561.10it/s]  \n",
      "train epoch[14/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 612.00it/s]  \n",
      "train epoch[15/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 618.26it/s]  \n",
      "train epoch[16/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 607.92it/s]  \n",
      "train epoch[17/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 595.55it/s]  \n",
      "train epoch[18/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 593.41it/s]  \n",
      "train epoch[19/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 647.47it/s]  \n",
      "train epoch[20/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 635.94it/s]  \n",
      "train epoch[21/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 595.59it/s]  \n",
      "train epoch[22/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 618.16it/s]  \n",
      "train epoch[23/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 595.99it/s]  \n",
      "train epoch[24/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 612.00it/s]  \n",
      "train epoch[25/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 590.39it/s]  \n",
      "train epoch[26/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 603.37it/s]  \n",
      "train epoch[27/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 668.45it/s]  \n",
      "train epoch[28/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 651.08it/s]  \n",
      "train epoch[29/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 622.65it/s]  \n",
      "train epoch[30/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 617.17it/s]  \n",
      "train epoch[31/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 616.91it/s]  \n",
      "train epoch[32/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 616.70it/s]  \n",
      "train epoch[33/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 628.67it/s]  \n",
      "train epoch[34/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 688.79it/s]  \n",
      "train epoch[35/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 725.64it/s]  \n",
      "train epoch[36/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 643.91it/s]  \n",
      "train epoch[37/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 634.96it/s]  \n",
      "train epoch[38/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 643.67it/s]  \n",
      "train epoch[39/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 638.28it/s]  \n",
      "train epoch[40/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 641.42it/s]  \n",
      "train epoch[41/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 638.92it/s]  \n",
      "train epoch[42/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 623.53it/s] \n",
      "train epoch[43/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 634.19it/s]  \n",
      "train epoch[44/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 663.27it/s]  \n",
      "train epoch[45/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 678.53it/s]  \n",
      "train epoch[46/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 684.62it/s]  \n",
      "train epoch[47/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 712.34it/s]  \n",
      "train epoch[48/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 724.03it/s]  \n",
      "train epoch[49/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 691.91it/s]  \n",
      "train epoch[50/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 690.61it/s]  \n",
      "train epoch[51/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 688.79it/s]  \n",
      "train epoch[52/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 666.92it/s]  \n",
      "train epoch[53/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 630.25it/s]  \n",
      "train epoch[54/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 628.45it/s]  \n",
      "train epoch[55/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 666.76it/s]  \n",
      "train epoch[56/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 682.76it/s]  \n",
      "train epoch[57/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 668.45it/s]  \n",
      "train epoch[58/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 660.14it/s]  \n",
      "train epoch[59/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 665.40it/s]  \n",
      "train epoch[60/60] loss:168.500: 100%|██████████| 1754/1754 [00:02<00:00, 667.15it/s]  \n",
      "100%|██████████| 439/439 [00:00<00:00, 2347.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE= -1.2348134381480818\n",
      "MAE= 72.79566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型调用过程\n",
    "# 1.初始化配置\n",
    "config = Config(train_size=0.8)\n",
    "# 2.初始化模型超参数\n",
    "nn_config = NN_config(learning_rate=1e-3, epochs=60)\n",
    "# 3.根据使用模型修改名称\n",
    "config.modify_params('model_name', 'lstm')\n",
    "# 4.读取数据\n",
    "data = read_data(config.data_path)\n",
    "# 5.划分训练集、测试集\n",
    "x_train, y_train, x_test, y_test = split_data(data, config.timestep, config.train_size)\n",
    "# 6.将数据格式转为tensor\n",
    "x_test_tensor, y_test_tensor, train_loader, test_loader = tensor_load(x_train, y_train, x_test, y_test, nn_config.batch_size)\n",
    "# 7.模型训练\n",
    "train_loss_plot, test_loss_plot = LSTM_model(train_loader, test_loader, config, nn_config)\n",
    "# 8.模型预测\n",
    "y_pre, NSE, MAE = prediction(x_test_tensor, y_test_tensor, config)\n",
    "# 9.结果保存\n",
    "save_results(y_test_tensor, y_pre, config)\n",
    "# 10.绘图展示（4.中展示）\n",
    "print(\"NSE=\", NSE)\n",
    "print(\"MAE=\", MAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建网络超参数类\n",
    "class NN_config:\n",
    "    def __init__(self, batch_size=8, feature_size=11, hidden_size=128, num_layers=2, output_size=1, epochs=50, learning_rate=1e-8, best_loss=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_loss = best_loss\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'batch_size':\n",
    "            self.batch_size = value\n",
    "        elif param == 'feature_size':\n",
    "            self.feature_size = value\n",
    "        elif param == 'hidden_size':\n",
    "            self.hidden_size = value\n",
    "        elif param == 'num_layers':\n",
    "            self.num_layers = value\n",
    "        elif param == 'output_size':\n",
    "            self.output_size = value\n",
    "        elif param == 'epochs':\n",
    "            self.epochs = value\n",
    "        elif param == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif param == 'best_loss':\n",
    "            self.best_loss = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')\n",
    "        \n",
    "# 定义GRU网络\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, num_layers, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # feature_size为特征维度，就是每个时间点对应的特征数量\n",
    "        self.gru = nn.GRU(feature_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.shape[0]  # 获取批次大小\n",
    "        if hidden is None:\n",
    "            h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n",
    "        else:\n",
    "            h_0 = hidden\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        output, h_0 = self.gru(x, h_0)\n",
    "        output = self.fc(output)  # 形状为batch_size * timestep, 1\n",
    "        return output[:, -1, :]  # 只需要返回最后一个时间切片的数据\n",
    "    \n",
    "# 模型训练并保存\n",
    "def GRU_model(train_loader, test_loader, config, nn_config):\n",
    "    # 选择训练硬件设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义RNN网络\n",
    "    model = GRU(nn_config.feature_size, nn_config.hidden_size, nn_config.num_layers, nn_config.output_size).to(device)\n",
    "    # 定义损失函数\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nn_config.learning_rate)\n",
    "    # 模型训练\n",
    "    train_loss_plot = []\n",
    "    test_loss_plot = []\n",
    "    for epoch in range(nn_config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        loss_plot = []\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_train_pred = model(x_train)\n",
    "            loss = loss_function(y_train_pred, y_train.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_plot.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, nn_config.epochs, loss)\n",
    "        train_loss_plot.append(min(loss_plot))\n",
    "            \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = loss_function(y_test_pred, y_test.reshape(-1, 1))\n",
    "            test_loss_plot.append(test_loss)\n",
    "    \n",
    "    save_path = './Model/{}.pth'.format(config.model_name)\n",
    "    if test_loss < nn_config.best_loss:\n",
    "        nn_config.modify('best_loss', test_loss)\n",
    "        torch.save(model, save_path)\n",
    "    else:\n",
    "        torch.save(model, save_path)\n",
    "    return train_loss_plot, test_loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/60] loss:74.002: 100%|██████████| 1754/1754 [00:02<00:00, 597.67it/s]   \n",
      "train epoch[2/60] loss:57.729: 100%|██████████| 1754/1754 [00:02<00:00, 705.07it/s]  \n",
      "train epoch[3/60] loss:56.997: 100%|██████████| 1754/1754 [00:02<00:00, 759.52it/s]  \n",
      "train epoch[4/60] loss:52.825: 100%|██████████| 1754/1754 [00:02<00:00, 736.54it/s] \n",
      "train epoch[5/60] loss:48.643: 100%|██████████| 1754/1754 [00:02<00:00, 704.74it/s]  \n",
      "train epoch[6/60] loss:47.268: 100%|██████████| 1754/1754 [00:02<00:00, 661.36it/s]  \n",
      "train epoch[7/60] loss:49.914: 100%|██████████| 1754/1754 [00:02<00:00, 651.08it/s] \n",
      "train epoch[8/60] loss:51.582: 100%|██████████| 1754/1754 [00:02<00:00, 704.31it/s]  \n",
      "train epoch[9/60] loss:51.542: 100%|██████████| 1754/1754 [00:02<00:00, 721.75it/s]  \n",
      "train epoch[10/60] loss:53.055: 100%|██████████| 1754/1754 [00:02<00:00, 711.58it/s]  \n",
      "train epoch[11/60] loss:56.255: 100%|██████████| 1754/1754 [00:02<00:00, 701.56it/s]  \n",
      "train epoch[12/60] loss:57.751: 100%|██████████| 1754/1754 [00:02<00:00, 707.60it/s]  \n",
      "train epoch[13/60] loss:56.258: 100%|██████████| 1754/1754 [00:02<00:00, 704.51it/s]  \n",
      "train epoch[14/60] loss:55.633: 100%|██████████| 1754/1754 [00:02<00:00, 682.55it/s]  \n",
      "train epoch[15/60] loss:54.243: 100%|██████████| 1754/1754 [00:02<00:00, 698.97it/s] \n",
      "train epoch[16/60] loss:53.729: 100%|██████████| 1754/1754 [00:02<00:00, 692.50it/s] \n",
      "train epoch[17/60] loss:52.979: 100%|██████████| 1754/1754 [00:02<00:00, 713.47it/s]  \n",
      "train epoch[18/60] loss:53.456: 100%|██████████| 1754/1754 [00:02<00:00, 727.82it/s] \n",
      "train epoch[19/60] loss:53.755: 100%|██████████| 1754/1754 [00:02<00:00, 718.90it/s]  \n",
      "train epoch[20/60] loss:54.128: 100%|██████████| 1754/1754 [00:02<00:00, 719.56it/s] \n",
      "train epoch[21/60] loss:53.518: 100%|██████████| 1754/1754 [00:02<00:00, 740.21it/s] \n",
      "train epoch[22/60] loss:53.589: 100%|██████████| 1754/1754 [00:02<00:00, 701.41it/s]  \n",
      "train epoch[23/60] loss:52.315: 100%|██████████| 1754/1754 [00:02<00:00, 679.01it/s]  \n",
      "train epoch[24/60] loss:53.739: 100%|██████████| 1754/1754 [00:02<00:00, 712.65it/s]  \n",
      "train epoch[25/60] loss:51.723: 100%|██████████| 1754/1754 [00:02<00:00, 716.04it/s]  \n",
      "train epoch[26/60] loss:53.373: 100%|██████████| 1754/1754 [00:02<00:00, 729.91it/s] \n",
      "train epoch[27/60] loss:51.815: 100%|██████████| 1754/1754 [00:02<00:00, 735.08it/s] \n",
      "train epoch[28/60] loss:52.539: 100%|██████████| 1754/1754 [00:02<00:00, 713.07it/s]  \n",
      "train epoch[29/60] loss:50.841: 100%|██████████| 1754/1754 [00:02<00:00, 697.53it/s] \n",
      "train epoch[30/60] loss:52.094: 100%|██████████| 1754/1754 [00:02<00:00, 690.96it/s] \n",
      "train epoch[31/60] loss:50.231: 100%|██████████| 1754/1754 [00:02<00:00, 694.48it/s] \n",
      "train epoch[32/60] loss:50.952: 100%|██████████| 1754/1754 [00:02<00:00, 685.57it/s] \n",
      "train epoch[33/60] loss:50.214: 100%|██████████| 1754/1754 [00:02<00:00, 709.65it/s]  \n",
      "train epoch[34/60] loss:49.966: 100%|██████████| 1754/1754 [00:02<00:00, 748.94it/s]  \n",
      "train epoch[35/60] loss:49.202: 100%|██████████| 1754/1754 [00:02<00:00, 754.60it/s]  \n",
      "train epoch[36/60] loss:47.978: 100%|██████████| 1754/1754 [00:02<00:00, 757.02it/s] \n",
      "train epoch[37/60] loss:46.632: 100%|██████████| 1754/1754 [00:02<00:00, 807.43it/s]  \n",
      "train epoch[38/60] loss:46.220: 100%|██████████| 1754/1754 [00:02<00:00, 814.65it/s] \n",
      "train epoch[39/60] loss:44.446: 100%|██████████| 1754/1754 [00:02<00:00, 791.39it/s]  \n",
      "train epoch[40/60] loss:43.887: 100%|██████████| 1754/1754 [00:02<00:00, 716.56it/s] \n",
      "train epoch[41/60] loss:42.908: 100%|██████████| 1754/1754 [00:02<00:00, 735.67it/s] \n",
      "train epoch[42/60] loss:41.865: 100%|██████████| 1754/1754 [00:02<00:00, 704.74it/s]  \n",
      "train epoch[43/60] loss:41.575: 100%|██████████| 1754/1754 [00:02<00:00, 727.84it/s]  \n",
      "train epoch[44/60] loss:40.178: 100%|██████████| 1754/1754 [00:02<00:00, 720.87it/s]  \n",
      "train epoch[45/60] loss:40.522: 100%|██████████| 1754/1754 [00:02<00:00, 721.98it/s] \n",
      "train epoch[46/60] loss:39.960: 100%|██████████| 1754/1754 [00:02<00:00, 740.73it/s]  \n",
      "train epoch[47/60] loss:39.820: 100%|██████████| 1754/1754 [00:02<00:00, 751.23it/s] \n",
      "train epoch[48/60] loss:39.578: 100%|██████████| 1754/1754 [00:02<00:00, 668.85it/s]  \n",
      "train epoch[49/60] loss:39.451: 100%|██████████| 1754/1754 [00:02<00:00, 650.32it/s] \n",
      "train epoch[50/60] loss:38.804: 100%|██████████| 1754/1754 [00:02<00:00, 646.03it/s] \n",
      "train epoch[51/60] loss:38.773: 100%|██████████| 1754/1754 [00:02<00:00, 687.45it/s] \n",
      "train epoch[52/60] loss:40.712: 100%|██████████| 1754/1754 [00:02<00:00, 676.96it/s] \n",
      "train epoch[53/60] loss:38.023: 100%|██████████| 1754/1754 [00:02<00:00, 653.37it/s]  \n",
      "train epoch[54/60] loss:38.588: 100%|██████████| 1754/1754 [00:02<00:00, 710.20it/s]  \n",
      "train epoch[55/60] loss:38.369: 100%|██████████| 1754/1754 [00:02<00:00, 749.70it/s] \n",
      "train epoch[56/60] loss:37.444: 100%|██████████| 1754/1754 [00:02<00:00, 754.85it/s] \n",
      "train epoch[57/60] loss:35.595: 100%|██████████| 1754/1754 [00:02<00:00, 670.29it/s]  \n",
      "train epoch[58/60] loss:38.070: 100%|██████████| 1754/1754 [00:02<00:00, 695.76it/s]  \n",
      "train epoch[59/60] loss:35.703: 100%|██████████| 1754/1754 [00:02<00:00, 695.86it/s]  \n",
      "train epoch[60/60] loss:36.627: 100%|██████████| 1754/1754 [00:02<00:00, 720.86it/s] \n",
      "100%|██████████| 439/439 [00:00<00:00, 2420.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE= 0.9391883507143608\n",
      "MAE= 9.837735\n"
     ]
    }
   ],
   "source": [
    "# 模型调用过程\n",
    "# 1.初始化配置\n",
    "config = Config(train_size=0.8)\n",
    "# 2.初始化模型超参数\n",
    "nn_config = NN_config(batch_size=16, learning_rate=1e-3, epochs=60)\n",
    "# 3.根据使用模型修改名称\n",
    "config.modify_params('model_name', 'gru')\n",
    "# 4.读取数据\n",
    "data = read_data(config.data_path)\n",
    "# 5.划分训练集、测试集\n",
    "x_train, y_train, x_test, y_test = split_data(data, config.timestep, config.train_size)\n",
    "# 6.将数据格式转为tensor\n",
    "x_test_tensor, y_test_tensor, train_loader, test_loader = tensor_load(x_train, y_train, x_test, y_test, nn_config.batch_size)\n",
    "# 7.模型训练\n",
    "train_loss_plot, test_loss_plot = GRU_model(train_loader, test_loader, config, nn_config)\n",
    "# 8.模型预测\n",
    "y_pre, NSE, MAE = prediction(x_test_tensor, y_test_tensor, config)\n",
    "# 9.结果保存\n",
    "save_results(y_test_tensor, y_pre, config)\n",
    "# 10.绘图展示（4.中展示）\n",
    "print(\"NSE=\", NSE)\n",
    "print(\"MAE=\", MAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.卷积神经网络（CNN）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 传统CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建网络超参数类\n",
    "class NN_config:\n",
    "    def __init__(self, batch_size=16, feature_size=11, out_channels=[10, 10, 10], output_size=1, epochs=50, learning_rate=1e-8, best_loss=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.output_size = output_size\n",
    "        self.out_channels = out_channels\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_loss = best_loss\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'batch_size':\n",
    "            self.batch_size = value\n",
    "        elif param == 'feature_size':\n",
    "            self.feature_size = value\n",
    "        elif param == 'out_channels':\n",
    "            self.out_channels = value\n",
    "        elif param == 'output_size':\n",
    "            self.output_size = value\n",
    "        elif param == 'epochs':\n",
    "            self.epochs = value\n",
    "        elif param == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif param == 'best_loss':\n",
    "            self.best_loss = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')\n",
    "    \n",
    "# 定义CNN网络\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, feature_size, out_channels, output_size, kernel_size=3, stride=2, padding=0):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(feature_size, out_channels[0], kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.conv1d_2 = nn.Conv1d(out_channels[0], out_channels[1], kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.conv1d_3 = nn.Conv1d(out_channels[1], out_channels[2], kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.maxpool1 = nn.AdaptiveMaxPool1d(output_size=20)\n",
    "        self.maxpool2 = nn.AdaptiveMaxPool1d(output_size=15)\n",
    "        self.maxpool3 = nn.AdaptiveMaxPool1d(output_size=10)\n",
    "        self.fc = nn.Linear(out_channels[2] * 10, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1d_1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = x.reshape(-1, x.shape[1] * x.shape[2])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 模型训练并保存\n",
    "def CNN_model(train_loader, test_loader, config, nn_config):\n",
    "    # 选择训练硬件设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义CNN网络\n",
    "    model = CNN(nn_config.feature_size, nn_config.out_channels, nn_config.output_size).to(device)\n",
    "    # 定义损失函数\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nn_config.learning_rate)        \n",
    "    # 模型训练\n",
    "    train_loss_plot = []\n",
    "    test_loss_plot = []\n",
    "    for epoch in range(nn_config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        loss_plot = []\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_train_pred = model(x_train.transpose(1, 2))\n",
    "            loss = loss_function(y_train_pred, y_train.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_plot.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, nn_config.epochs, loss)\n",
    "        train_loss_plot.append(min(loss_plot))\n",
    "    # 模型验证\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            y_test_pred = model(x_test.transpose(1, 2))\n",
    "            test_loss = loss_function(y_test_pred, y_test.reshape(-1, 1))\n",
    "            test_loss_plot.append(test_loss)\n",
    "                \n",
    "    save_path = './Model/{}.pth'.format(config.model_name)\n",
    "    if test_loss < nn_config.best_loss:\n",
    "        nn_config.modify('best_loss', test_loss)\n",
    "        torch.save(model, save_path)\n",
    "    else:\n",
    "        torch.save(model, save_path)\n",
    "    return train_loss_plot, test_loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/150] loss:355.396: 100%|██████████| 3506/3506 [00:04<00:00, 725.18it/s]  \n",
      "train epoch[2/150] loss:335.899: 100%|██████████| 3506/3506 [00:04<00:00, 734.46it/s]  \n",
      "train epoch[3/150] loss:313.810: 100%|██████████| 3506/3506 [00:04<00:00, 835.17it/s]  \n",
      "train epoch[4/150] loss:289.700: 100%|██████████| 3506/3506 [00:04<00:00, 823.13it/s]  \n",
      "train epoch[5/150] loss:263.881: 100%|██████████| 3506/3506 [00:04<00:00, 786.57it/s]  \n",
      "train epoch[6/150] loss:237.120: 100%|██████████| 3506/3506 [00:04<00:00, 741.20it/s]  \n",
      "train epoch[7/150] loss:210.837: 100%|██████████| 3506/3506 [00:05<00:00, 691.14it/s]  \n",
      "train epoch[8/150] loss:187.172: 100%|██████████| 3506/3506 [00:04<00:00, 731.08it/s]  \n",
      "train epoch[9/150] loss:168.581: 100%|██████████| 3506/3506 [00:04<00:00, 799.75it/s] \n",
      "train epoch[10/150] loss:156.531: 100%|██████████| 3506/3506 [00:04<00:00, 742.40it/s] \n",
      "train epoch[11/150] loss:150.129: 100%|██████████| 3506/3506 [00:04<00:00, 767.41it/s] \n",
      "train epoch[12/150] loss:147.229: 100%|██████████| 3506/3506 [00:04<00:00, 827.71it/s]  \n",
      "train epoch[13/150] loss:146.164: 100%|██████████| 3506/3506 [00:04<00:00, 729.65it/s]  \n",
      "train epoch[14/150] loss:146.010: 100%|██████████| 3506/3506 [00:04<00:00, 743.19it/s]  \n",
      "train epoch[15/150] loss:146.310: 100%|██████████| 3506/3506 [00:04<00:00, 705.18it/s] \n",
      "train epoch[16/150] loss:146.852: 100%|██████████| 3506/3506 [00:04<00:00, 757.99it/s] \n",
      "train epoch[17/150] loss:147.509: 100%|██████████| 3506/3506 [00:04<00:00, 766.39it/s] \n",
      "train epoch[18/150] loss:148.227: 100%|██████████| 3506/3506 [00:04<00:00, 748.71it/s] \n",
      "train epoch[19/150] loss:148.980: 100%|██████████| 3506/3506 [00:05<00:00, 668.90it/s]  \n",
      "train epoch[20/150] loss:149.755: 100%|██████████| 3506/3506 [00:04<00:00, 747.48it/s]  \n",
      "train epoch[21/150] loss:150.546: 100%|██████████| 3506/3506 [00:04<00:00, 779.60it/s]  \n",
      "train epoch[22/150] loss:151.350: 100%|██████████| 3506/3506 [00:04<00:00, 750.11it/s] \n",
      "train epoch[23/150] loss:152.165: 100%|██████████| 3506/3506 [00:04<00:00, 757.02it/s] \n",
      "train epoch[24/150] loss:152.992: 100%|██████████| 3506/3506 [00:04<00:00, 754.30it/s]  \n",
      "train epoch[25/150] loss:153.829: 100%|██████████| 3506/3506 [00:04<00:00, 738.53it/s]  \n",
      "train epoch[26/150] loss:154.677: 100%|██████████| 3506/3506 [00:04<00:00, 800.37it/s]  \n",
      "train epoch[27/150] loss:155.535: 100%|██████████| 3506/3506 [00:04<00:00, 800.76it/s] \n",
      "train epoch[28/150] loss:156.407: 100%|██████████| 3506/3506 [00:04<00:00, 761.59it/s] \n",
      "train epoch[29/150] loss:157.294: 100%|██████████| 3506/3506 [00:04<00:00, 809.98it/s]  \n",
      "train epoch[30/150] loss:158.191: 100%|██████████| 3506/3506 [00:04<00:00, 790.20it/s] \n",
      "train epoch[31/150] loss:159.099: 100%|██████████| 3506/3506 [00:04<00:00, 767.50it/s]  \n",
      "train epoch[32/150] loss:160.017: 100%|██████████| 3506/3506 [00:04<00:00, 735.78it/s] \n",
      "train epoch[33/150] loss:160.945: 100%|██████████| 3506/3506 [00:04<00:00, 735.62it/s] \n",
      "train epoch[34/150] loss:161.882: 100%|██████████| 3506/3506 [00:04<00:00, 746.77it/s] \n",
      "train epoch[35/150] loss:162.830: 100%|██████████| 3506/3506 [00:04<00:00, 773.98it/s]  \n",
      "train epoch[36/150] loss:163.786: 100%|██████████| 3506/3506 [00:04<00:00, 786.88it/s]  \n",
      "train epoch[37/150] loss:164.752: 100%|██████████| 3506/3506 [00:04<00:00, 711.39it/s]  \n",
      "train epoch[38/150] loss:165.726: 100%|██████████| 3506/3506 [00:04<00:00, 808.04it/s] \n",
      "train epoch[39/150] loss:166.711: 100%|██████████| 3506/3506 [00:04<00:00, 876.30it/s] \n",
      "train epoch[40/150] loss:167.708: 100%|██████████| 3506/3506 [00:04<00:00, 828.13it/s]  \n",
      "train epoch[41/150] loss:168.713: 100%|██████████| 3506/3506 [00:04<00:00, 794.18it/s]  \n",
      "train epoch[42/150] loss:169.724: 100%|██████████| 3506/3506 [00:04<00:00, 752.42it/s] \n",
      "train epoch[43/150] loss:170.741: 100%|██████████| 3506/3506 [00:04<00:00, 776.70it/s] \n",
      "train epoch[44/150] loss:171.764: 100%|██████████| 3506/3506 [00:04<00:00, 801.83it/s] \n",
      "train epoch[45/150] loss:172.792: 100%|██████████| 3506/3506 [00:04<00:00, 809.74it/s]  \n",
      "train epoch[46/150] loss:173.824: 100%|██████████| 3506/3506 [00:04<00:00, 750.62it/s] \n",
      "train epoch[47/150] loss:174.862: 100%|██████████| 3506/3506 [00:04<00:00, 727.21it/s] \n",
      "train epoch[48/150] loss:175.903: 100%|██████████| 3506/3506 [00:04<00:00, 802.74it/s]  \n",
      "train epoch[49/150] loss:176.946: 100%|██████████| 3506/3506 [00:04<00:00, 773.59it/s] \n",
      "train epoch[50/150] loss:177.990: 100%|██████████| 3506/3506 [00:04<00:00, 774.44it/s] \n",
      "train epoch[51/150] loss:179.033: 100%|██████████| 3506/3506 [00:04<00:00, 733.72it/s] \n",
      "train epoch[52/150] loss:180.076: 100%|██████████| 3506/3506 [00:04<00:00, 769.29it/s] \n",
      "train epoch[53/150] loss:181.116: 100%|██████████| 3506/3506 [00:04<00:00, 803.44it/s] \n",
      "train epoch[54/150] loss:182.154: 100%|██████████| 3506/3506 [00:04<00:00, 844.66it/s] \n",
      "train epoch[55/150] loss:183.186: 100%|██████████| 3506/3506 [00:04<00:00, 841.17it/s]  \n",
      "train epoch[56/150] loss:184.214: 100%|██████████| 3506/3506 [00:04<00:00, 717.40it/s]  \n",
      "train epoch[57/150] loss:185.234: 100%|██████████| 3506/3506 [00:04<00:00, 812.52it/s] \n",
      "train epoch[58/150] loss:186.247: 100%|██████████| 3506/3506 [00:04<00:00, 865.25it/s] \n",
      "train epoch[59/150] loss:187.251: 100%|██████████| 3506/3506 [00:04<00:00, 784.38it/s] \n",
      "train epoch[60/150] loss:188.244: 100%|██████████| 3506/3506 [00:04<00:00, 766.55it/s] \n",
      "train epoch[61/150] loss:189.226: 100%|██████████| 3506/3506 [00:04<00:00, 740.90it/s] \n",
      "train epoch[62/150] loss:190.195: 100%|██████████| 3506/3506 [00:04<00:00, 764.56it/s] \n",
      "train epoch[63/150] loss:191.150: 100%|██████████| 3506/3506 [00:04<00:00, 794.65it/s]  \n",
      "train epoch[64/150] loss:192.089: 100%|██████████| 3506/3506 [00:04<00:00, 846.17it/s] \n",
      "train epoch[65/150] loss:193.012: 100%|██████████| 3506/3506 [00:04<00:00, 807.89it/s] \n",
      "train epoch[66/150] loss:193.917: 100%|██████████| 3506/3506 [00:04<00:00, 779.75it/s] \n",
      "train epoch[67/150] loss:194.803: 100%|██████████| 3506/3506 [00:04<00:00, 813.93it/s]  \n",
      "train epoch[68/150] loss:195.669: 100%|██████████| 3506/3506 [00:04<00:00, 813.31it/s] \n",
      "train epoch[69/150] loss:196.513: 100%|██████████| 3506/3506 [00:04<00:00, 819.05it/s]  \n",
      "train epoch[70/150] loss:197.335: 100%|██████████| 3506/3506 [00:04<00:00, 787.69it/s] \n",
      "train epoch[71/150] loss:198.133: 100%|██████████| 3506/3506 [00:04<00:00, 778.69it/s] \n",
      "train epoch[72/150] loss:198.906: 100%|██████████| 3506/3506 [00:04<00:00, 760.78it/s] \n",
      "train epoch[73/150] loss:199.655: 100%|██████████| 3506/3506 [00:04<00:00, 857.09it/s] \n",
      "train epoch[74/150] loss:200.377: 100%|██████████| 3506/3506 [00:04<00:00, 822.81it/s] \n",
      "train epoch[75/150] loss:201.073: 100%|██████████| 3506/3506 [00:04<00:00, 731.65it/s] \n",
      "train epoch[76/150] loss:201.740: 100%|██████████| 3506/3506 [00:04<00:00, 816.08it/s]  \n",
      "train epoch[77/150] loss:202.380: 100%|██████████| 3506/3506 [00:04<00:00, 791.46it/s] \n",
      "train epoch[78/150] loss:202.990: 100%|██████████| 3506/3506 [00:04<00:00, 792.10it/s] \n",
      "train epoch[79/150] loss:203.572: 100%|██████████| 3506/3506 [00:04<00:00, 778.45it/s] \n",
      "train epoch[80/150] loss:204.123: 100%|██████████| 3506/3506 [00:04<00:00, 753.92it/s] \n",
      "train epoch[81/150] loss:204.642: 100%|██████████| 3506/3506 [00:04<00:00, 780.50it/s] \n",
      "train epoch[82/150] loss:205.135: 100%|██████████| 3506/3506 [00:04<00:00, 838.39it/s] \n",
      "train epoch[83/150] loss:205.601: 100%|██████████| 3506/3506 [00:03<00:00, 893.80it/s] \n",
      "train epoch[84/150] loss:206.036: 100%|██████████| 3506/3506 [00:04<00:00, 745.62it/s]  \n",
      "train epoch[85/150] loss:206.441: 100%|██████████| 3506/3506 [00:04<00:00, 752.59it/s]  \n",
      "train epoch[86/150] loss:206.815: 100%|██████████| 3506/3506 [00:04<00:00, 801.61it/s] \n",
      "train epoch[87/150] loss:207.161: 100%|██████████| 3506/3506 [00:04<00:00, 746.65it/s]  \n",
      "train epoch[88/150] loss:207.476: 100%|██████████| 3506/3506 [00:04<00:00, 747.42it/s] \n",
      "train epoch[89/150] loss:207.763: 100%|██████████| 3506/3506 [00:04<00:00, 748.38it/s] \n",
      "train epoch[90/150] loss:208.020: 100%|██████████| 3506/3506 [00:04<00:00, 752.40it/s] \n",
      "train epoch[91/150] loss:208.249: 100%|██████████| 3506/3506 [00:04<00:00, 834.72it/s] \n",
      "train epoch[92/150] loss:208.449: 100%|██████████| 3506/3506 [00:03<00:00, 891.40it/s]  \n",
      "train epoch[93/150] loss:208.622: 100%|██████████| 3506/3506 [00:04<00:00, 807.81it/s] \n",
      "train epoch[94/150] loss:208.768: 100%|██████████| 3506/3506 [00:05<00:00, 699.60it/s] \n",
      "train epoch[95/150] loss:208.887: 100%|██████████| 3506/3506 [00:04<00:00, 830.53it/s] \n",
      "train epoch[96/150] loss:208.981: 100%|██████████| 3506/3506 [00:04<00:00, 837.40it/s] \n",
      "train epoch[97/150] loss:209.049: 100%|██████████| 3506/3506 [00:04<00:00, 806.01it/s] \n",
      "train epoch[98/150] loss:209.093: 100%|██████████| 3506/3506 [00:04<00:00, 755.72it/s] \n",
      "train epoch[99/150] loss:209.114: 100%|██████████| 3506/3506 [00:04<00:00, 740.57it/s] \n",
      "train epoch[100/150] loss:209.111: 100%|██████████| 3506/3506 [00:04<00:00, 731.00it/s]  \n",
      "train epoch[101/150] loss:209.086: 100%|██████████| 3506/3506 [00:04<00:00, 772.27it/s] \n",
      "train epoch[102/150] loss:209.039: 100%|██████████| 3506/3506 [00:04<00:00, 808.49it/s] \n",
      "train epoch[103/150] loss:208.971: 100%|██████████| 3506/3506 [00:04<00:00, 730.17it/s] \n",
      "train epoch[104/150] loss:208.883: 100%|██████████| 3506/3506 [00:04<00:00, 780.11it/s] \n",
      "train epoch[105/150] loss:208.775: 100%|██████████| 3506/3506 [00:04<00:00, 824.27it/s] \n",
      "train epoch[106/150] loss:208.648: 100%|██████████| 3506/3506 [00:04<00:00, 789.68it/s] \n",
      "train epoch[107/150] loss:208.502: 100%|██████████| 3506/3506 [00:04<00:00, 758.66it/s] \n",
      "train epoch[108/150] loss:208.339: 100%|██████████| 3506/3506 [00:04<00:00, 739.53it/s]  \n",
      "train epoch[109/150] loss:208.160: 100%|██████████| 3506/3506 [00:04<00:00, 813.75it/s] \n",
      "train epoch[110/150] loss:207.964: 100%|██████████| 3506/3506 [00:04<00:00, 835.00it/s] \n",
      "train epoch[111/150] loss:207.753: 100%|██████████| 3506/3506 [00:03<00:00, 882.88it/s] \n",
      "train epoch[112/150] loss:207.527: 100%|██████████| 3506/3506 [00:04<00:00, 751.90it/s] \n",
      "train epoch[113/150] loss:207.287: 100%|██████████| 3506/3506 [00:04<00:00, 714.40it/s]  \n",
      "train epoch[114/150] loss:207.037: 100%|██████████| 3506/3506 [00:04<00:00, 794.99it/s] \n",
      "train epoch[115/150] loss:206.777: 100%|██████████| 3506/3506 [00:04<00:00, 752.45it/s] \n",
      "train epoch[116/150] loss:206.504: 100%|██████████| 3506/3506 [00:04<00:00, 750.91it/s] \n",
      "train epoch[117/150] loss:206.220: 100%|██████████| 3506/3506 [00:04<00:00, 772.91it/s] \n",
      "train epoch[118/150] loss:205.925: 100%|██████████| 3506/3506 [00:04<00:00, 769.57it/s] \n",
      "train epoch[119/150] loss:205.619: 100%|██████████| 3506/3506 [00:04<00:00, 806.26it/s] \n",
      "train epoch[120/150] loss:205.303: 100%|██████████| 3506/3506 [00:04<00:00, 874.11it/s] \n",
      "train epoch[121/150] loss:204.977: 100%|██████████| 3506/3506 [00:04<00:00, 827.70it/s] \n",
      "train epoch[122/150] loss:204.643: 100%|██████████| 3506/3506 [00:04<00:00, 720.66it/s] \n",
      "train epoch[123/150] loss:204.301: 100%|██████████| 3506/3506 [00:04<00:00, 786.96it/s] \n",
      "train epoch[124/150] loss:203.950: 100%|██████████| 3506/3506 [00:04<00:00, 812.33it/s] \n",
      "train epoch[125/150] loss:203.592: 100%|██████████| 3506/3506 [00:04<00:00, 781.20it/s] \n",
      "train epoch[126/150] loss:203.227: 100%|██████████| 3506/3506 [00:04<00:00, 771.43it/s] \n",
      "train epoch[127/150] loss:202.856: 100%|██████████| 3506/3506 [00:04<00:00, 775.10it/s] \n",
      "train epoch[128/150] loss:202.484: 100%|██████████| 3506/3506 [00:04<00:00, 761.66it/s] \n",
      "train epoch[129/150] loss:202.106: 100%|██████████| 3506/3506 [00:04<00:00, 787.67it/s] \n",
      "train epoch[130/150] loss:201.722: 100%|██████████| 3506/3506 [00:04<00:00, 814.30it/s] \n",
      "train epoch[131/150] loss:201.333: 100%|██████████| 3506/3506 [00:04<00:00, 710.06it/s] \n",
      "train epoch[132/150] loss:200.938: 100%|██████████| 3506/3506 [00:04<00:00, 752.49it/s] \n",
      "train epoch[133/150] loss:200.539: 100%|██████████| 3506/3506 [00:04<00:00, 833.79it/s] \n",
      "train epoch[134/150] loss:200.135: 100%|██████████| 3506/3506 [00:04<00:00, 826.56it/s] \n",
      "train epoch[135/150] loss:199.725: 100%|██████████| 3506/3506 [00:04<00:00, 833.51it/s] \n",
      "train epoch[136/150] loss:199.308: 100%|██████████| 3506/3506 [00:04<00:00, 863.09it/s] \n",
      "train epoch[137/150] loss:198.889: 100%|██████████| 3506/3506 [00:04<00:00, 851.38it/s] \n",
      "train epoch[138/150] loss:198.467: 100%|██████████| 3506/3506 [00:04<00:00, 819.91it/s] \n",
      "train epoch[139/150] loss:198.042: 100%|██████████| 3506/3506 [00:03<00:00, 935.50it/s] \n",
      "train epoch[140/150] loss:197.614: 100%|██████████| 3506/3506 [00:03<00:00, 915.01it/s] \n",
      "train epoch[141/150] loss:197.183: 100%|██████████| 3506/3506 [00:04<00:00, 793.41it/s]  \n",
      "train epoch[142/150] loss:196.750: 100%|██████████| 3506/3506 [00:04<00:00, 809.26it/s] \n",
      "train epoch[143/150] loss:196.314: 100%|██████████| 3506/3506 [00:04<00:00, 837.74it/s] \n",
      "train epoch[144/150] loss:195.875: 100%|██████████| 3506/3506 [00:04<00:00, 813.65it/s] \n",
      "train epoch[145/150] loss:195.435: 100%|██████████| 3506/3506 [00:04<00:00, 818.10it/s] \n",
      "train epoch[146/150] loss:194.992: 100%|██████████| 3506/3506 [00:03<00:00, 910.27it/s] \n",
      "train epoch[147/150] loss:194.548: 100%|██████████| 3506/3506 [00:03<00:00, 902.90it/s] \n",
      "train epoch[148/150] loss:194.103: 100%|██████████| 3506/3506 [00:04<00:00, 781.68it/s] \n",
      "train epoch[149/150] loss:193.656: 100%|██████████| 3506/3506 [00:04<00:00, 841.57it/s] \n",
      "train epoch[150/150] loss:193.209: 100%|██████████| 3506/3506 [00:03<00:00, 892.55it/s] \n",
      "100%|██████████| 877/877 [00:00<00:00, 2737.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE= 0.8185555120875392\n",
      "MAE= 17.525034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型调用过程\n",
    "# 1.初始化配置\n",
    "config = Config(train_size=0.8, timestep=12)\n",
    "# 2.初始化模型超参数\n",
    "nn_config = NN_config(batch_size=8, learning_rate=1e-6, epochs=150)\n",
    "# 3.根据使用模型修改名称\n",
    "config.modify_params('model_name', 'cnn')\n",
    "# 4.读取数据\n",
    "data = read_data(config.data_path)\n",
    "# 5.划分训练集、测试集\n",
    "x_train, y_train, x_test, y_test = split_data_cnn(data, config.timestep, config.train_size)\n",
    "# 6.将数据格式转为tensor\n",
    "x_test_tensor, y_test_tensor, train_loader, test_loader = tensor_load_cnn(x_train, y_train, x_test, y_test, nn_config.batch_size)\n",
    "# 7.模型训练\n",
    "train_loss_plot, test_loss_plot = CNN_model(train_loader, test_loader, config, nn_config)\n",
    "# 8.模型预测\n",
    "y_pre, NSE, MAE = prediction(x_test_tensor.transpose(1, 2), y_test_tensor, config)\n",
    "# 9.结果保存\n",
    "save_results_cnn(y_test_tensor.reshape(y_test_tensor.shape[0], 1), y_pre.reshape(-1), config)\n",
    "# 10.绘图展示（4.中展示）\n",
    "print(\"NSE=\", NSE)\n",
    "print(\"MAE=\", MAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建网络超参数类\n",
    "class NN_config:\n",
    "    def __init__(self, batch_size=16, feature_size=11, num_channels=[32, 64, 128, 256], output_size=1, epochs=50, learning_rate=1e-8, best_loss=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.output_size = output_size\n",
    "        self.num_channels = num_channels\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_loss = best_loss\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'batch_size':\n",
    "            self.batch_size = value\n",
    "        elif param == 'feature_size':\n",
    "            self.feature_size = value\n",
    "        elif param == 'num_channels':\n",
    "            self.num_channels = value\n",
    "        elif param == 'output_size':\n",
    "            self.output_size = value\n",
    "        elif param == 'epochs':\n",
    "            self.epochs = value\n",
    "        elif param == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif param == 'best_loss':\n",
    "            self.best_loss = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')\n",
    "\n",
    "# 定义TCN网络\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, feature_size, num_channels, output_size, kernel_size=3, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.output_size = output_size\n",
    "        self.num_channels = num_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.relu = nn.ReLU()        \n",
    "        # 定义卷积层和dropout层\n",
    "        self.model_list = nn.ModuleList([nn.Conv1d(feature_size, num_channels[0], kernel_size), \n",
    "                                         nn.Conv1d(num_channels[0], num_channels[1], kernel_size),\n",
    "                                         nn.Conv1d(num_channels[1], num_channels[2], kernel_size),\n",
    "                                         nn.Conv1d(num_channels[2], num_channels[3], kernel_size)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 将数据维度从（batch_size, seq_len, input_size）变成（batch_size, input_size, seq_len）\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # 通过卷积层和dropout进行特征提取\n",
    "        for layer in self.model_list:\n",
    "            x = layer(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        # 将卷积层的输出求平均并输入全连接层得到最终输出\n",
    "        x = x.mean(dim=2)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# 模型训练并保存\n",
    "def TCN_model(train_loader, test_loader, config, nn_config):\n",
    "    # 选择训练硬件设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义TCN网络\n",
    "    model = TCN(nn_config.feature_size, nn_config.num_channels, nn_config.output_size).to(device)\n",
    "    # 定义损失函数\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nn_config.learning_rate)\n",
    "    # 模型训练\n",
    "    train_loss_plot = []\n",
    "    test_loss_plot = []\n",
    "    for epoch in range(nn_config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        loss_plot = []\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_train_pred = model(x_train)\n",
    "            loss = loss_function(y_train_pred, y_train.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_plot.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, nn_config.epochs, loss)\n",
    "        train_loss_plot.append(min(loss_plot))\n",
    "    # 模型验证\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = loss_function(y_test_pred, y_test.reshape(-1 ,1))\n",
    "            test_loss_plot.append(test_loss)\n",
    "            \n",
    "    save_path = './Model/{}.pth'.format(config.model_name)\n",
    "    if test_loss < nn_config.best_loss:\n",
    "        nn_config.modify('best_loss', test_loss)\n",
    "        torch.save(model, save_path)\n",
    "    else:\n",
    "        torch.save(model, save_path)\n",
    "    return train_loss_plot, test_loss_plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/50] loss:395.622: 100%|██████████| 3506/3506 [00:06<00:00, 559.44it/s]  \n",
      "train epoch[2/50] loss:393.833: 100%|██████████| 3506/3506 [00:06<00:00, 567.31it/s]  \n",
      "train epoch[3/50] loss:393.730: 100%|██████████| 3506/3506 [00:06<00:00, 549.33it/s]  \n",
      "train epoch[4/50] loss:390.117: 100%|██████████| 3506/3506 [00:06<00:00, 548.86it/s]  \n",
      "train epoch[5/50] loss:387.179: 100%|██████████| 3506/3506 [00:06<00:00, 563.74it/s]  \n",
      "train epoch[6/50] loss:382.287: 100%|██████████| 3506/3506 [00:06<00:00, 568.60it/s]  \n",
      "train epoch[7/50] loss:378.810: 100%|██████████| 3506/3506 [00:06<00:00, 556.12it/s]  \n",
      "train epoch[8/50] loss:378.193: 100%|██████████| 3506/3506 [00:06<00:00, 563.84it/s]  \n",
      "train epoch[9/50] loss:378.220: 100%|██████████| 3506/3506 [00:06<00:00, 562.14it/s]  \n",
      "train epoch[10/50] loss:372.208: 100%|██████████| 3506/3506 [00:06<00:00, 568.44it/s]  \n",
      "train epoch[11/50] loss:364.857: 100%|██████████| 3506/3506 [00:06<00:00, 541.81it/s]  \n",
      "train epoch[12/50] loss:359.735: 100%|██████████| 3506/3506 [00:06<00:00, 558.72it/s]  \n",
      "train epoch[13/50] loss:351.651: 100%|██████████| 3506/3506 [00:06<00:00, 549.59it/s]  \n",
      "train epoch[14/50] loss:347.629: 100%|██████████| 3506/3506 [00:06<00:00, 551.09it/s]  \n",
      "train epoch[15/50] loss:344.426: 100%|██████████| 3506/3506 [00:06<00:00, 565.90it/s]  \n",
      "train epoch[16/50] loss:347.546: 100%|██████████| 3506/3506 [00:05<00:00, 644.35it/s]  \n",
      "train epoch[17/50] loss:339.448: 100%|██████████| 3506/3506 [00:05<00:00, 666.22it/s]  \n",
      "train epoch[18/50] loss:323.228: 100%|██████████| 3506/3506 [00:06<00:00, 553.23it/s]  \n",
      "train epoch[19/50] loss:324.191: 100%|██████████| 3506/3506 [00:05<00:00, 633.92it/s]  \n",
      "train epoch[20/50] loss:303.104: 100%|██████████| 3506/3506 [00:05<00:00, 663.52it/s]  \n",
      "train epoch[21/50] loss:301.218: 100%|██████████| 3506/3506 [00:05<00:00, 646.14it/s]  \n",
      "train epoch[22/50] loss:316.388: 100%|██████████| 3506/3506 [00:05<00:00, 588.63it/s]  \n",
      "train epoch[23/50] loss:288.947: 100%|██████████| 3506/3506 [00:06<00:00, 564.31it/s]  \n",
      "train epoch[24/50] loss:282.591: 100%|██████████| 3506/3506 [00:06<00:00, 565.97it/s]  \n",
      "train epoch[25/50] loss:268.250: 100%|██████████| 3506/3506 [00:06<00:00, 568.79it/s]  \n",
      "train epoch[26/50] loss:254.748: 100%|██████████| 3506/3506 [00:05<00:00, 601.91it/s]  \n",
      "train epoch[27/50] loss:244.753: 100%|██████████| 3506/3506 [00:05<00:00, 639.97it/s]  \n",
      "train epoch[28/50] loss:230.767: 100%|██████████| 3506/3506 [00:05<00:00, 619.99it/s]  \n",
      "train epoch[29/50] loss:227.926: 100%|██████████| 3506/3506 [00:06<00:00, 559.63it/s]  \n",
      "train epoch[30/50] loss:200.149: 100%|██████████| 3506/3506 [00:05<00:00, 617.94it/s]  \n",
      "train epoch[31/50] loss:199.835: 100%|██████████| 3506/3506 [00:05<00:00, 638.93it/s]  \n",
      "train epoch[32/50] loss:197.313: 100%|██████████| 3506/3506 [00:05<00:00, 600.19it/s]  \n",
      "train epoch[33/50] loss:195.272: 100%|██████████| 3506/3506 [00:06<00:00, 581.05it/s]  \n",
      "train epoch[34/50] loss:154.267: 100%|██████████| 3506/3506 [00:05<00:00, 631.47it/s]  \n",
      "train epoch[35/50] loss:141.956: 100%|██████████| 3506/3506 [00:05<00:00, 619.66it/s]  \n",
      "train epoch[36/50] loss:175.555: 100%|██████████| 3506/3506 [00:05<00:00, 589.24it/s]  \n",
      "train epoch[37/50] loss:130.774: 100%|██████████| 3506/3506 [00:06<00:00, 574.46it/s] \n",
      "train epoch[38/50] loss:127.597: 100%|██████████| 3506/3506 [00:05<00:00, 620.48it/s]  \n",
      "train epoch[39/50] loss:136.821: 100%|██████████| 3506/3506 [00:05<00:00, 664.18it/s]  \n",
      "train epoch[40/50] loss:103.398: 100%|██████████| 3506/3506 [00:05<00:00, 604.15it/s]  \n",
      "train epoch[41/50] loss:126.289: 100%|██████████| 3506/3506 [00:06<00:00, 554.11it/s]  \n",
      "train epoch[42/50] loss:127.979: 100%|██████████| 3506/3506 [00:05<00:00, 607.46it/s]  \n",
      "train epoch[43/50] loss:88.597: 100%|██████████| 3506/3506 [00:05<00:00, 611.05it/s]   \n",
      "train epoch[44/50] loss:91.083: 100%|██████████| 3506/3506 [00:05<00:00, 613.93it/s]   \n",
      "train epoch[45/50] loss:108.060: 100%|██████████| 3506/3506 [00:05<00:00, 616.57it/s] \n",
      "train epoch[46/50] loss:92.315: 100%|██████████| 3506/3506 [00:05<00:00, 615.07it/s]  \n",
      "train epoch[47/50] loss:90.599: 100%|██████████| 3506/3506 [00:05<00:00, 589.57it/s]  \n",
      "train epoch[48/50] loss:89.861: 100%|██████████| 3506/3506 [00:05<00:00, 585.55it/s]   \n",
      "train epoch[49/50] loss:105.239: 100%|██████████| 3506/3506 [00:05<00:00, 601.33it/s] \n",
      "train epoch[50/50] loss:106.370: 100%|██████████| 3506/3506 [00:05<00:00, 598.94it/s]  \n",
      "100%|██████████| 877/877 [00:00<00:00, 2088.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE= 0.6220382283283994\n",
      "MAE= 27.732985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型调用过程\n",
    "# 1.初始化配置\n",
    "config = Config(train_size=0.8, timestep=12)\n",
    "# 2.初始化模型超参数\n",
    "nn_config = NN_config(batch_size=8, learning_rate=1e-7, epochs=50)\n",
    "# 3.根据使用模型修改名称\n",
    "config.modify_params('model_name', 'TCN')\n",
    "# 4.读取数据\n",
    "data = read_data(config.data_path)\n",
    "# 5.划分训练集、测试集\n",
    "x_train, y_train, x_test, y_test = split_data_cnn(data, config.timestep, config.train_size)\n",
    "# 6.将数据格式转为tensor\n",
    "x_test_tensor, y_test_tensor, train_loader, test_loader = tensor_load_cnn(x_train, y_train, x_test, y_test, nn_config.batch_size)\n",
    "# 7.模型训练\n",
    "train_loss_plot, test_loss_plot = TCN_model(train_loader, test_loader, config, nn_config)\n",
    "# 8.模型预测\n",
    "y_pre, NSE, MAE = prediction(x_test_tensor, y_test_tensor, config)\n",
    "# 9.结果保存\n",
    "save_results_cnn(y_test_tensor, y_pre, config)\n",
    "# 10.绘图展示（4.中展示）\n",
    "print(\"NSE=\", NSE)\n",
    "print(\"MAE=\", MAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.注意力机制（Attention）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建网络超参数类\n",
    "class NN_config:\n",
    "    def __init__(self, batch_size=16, feature_size=11, hidden_size=256, num_layers=2, output_size=1, epochs=50, learning_rate=1e-8, best_loss=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_loss = best_loss\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'batch_size':\n",
    "            self.batch_size = value\n",
    "        elif param == 'feature_size':\n",
    "            self.feature_size = value\n",
    "        elif param == 'hidden_size':\n",
    "            self.hidden_size = value\n",
    "        elif param == 'num_layers':\n",
    "            self.num_layers = value\n",
    "        elif param == 'output_size':\n",
    "            self.output_size = value\n",
    "        elif param == 'epochs':\n",
    "            self.epochs = value\n",
    "        elif param == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif param == 'best_loss':\n",
    "            self.best_loss = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')\n",
    "\n",
    "# 定义位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 初始化Shape为(max_len, d_model)的positional encoding(pe)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor[[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里是sin和cos括号中的内容，通过e和ln进行交换\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# 定义Transformer网络\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, feature_size, output_size, feedforward_dim=32, num_head=1, transformer_num_layers=1, dropout=0.3, max_len=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.hidden_size = hidden_size # 隐层大小\n",
    "        self.num_layers = num_layers # lstm层数\n",
    "        # feature_size为特征维度，就是每个时间点对应的特征数量\n",
    "        self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n",
    "        # 位置编码层\n",
    "        self.position_encoding = PositionalEncoding(hidden_size, dropout, max_len)\n",
    "        # 编码层\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(hidden_size, num_head, feedforward_dim, dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, transformer_num_layers)\n",
    "        # 输出层\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.shape[0]\n",
    "        # 初始化隐层状态\n",
    "        if hidden is None:\n",
    "            h_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n",
    "            c_0 = x.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "        # LSTM运算    \n",
    "        output, (h_0, c_0) = self.lstm(x, (h_0, c_0))\n",
    "        # 维度为【序列长度，批次，嵌入向量维度】\n",
    "        output = self.position_encoding(output)\n",
    "        output = self.transformer(output)\n",
    "        # 将每个数据的输出向量取均值，也可以随意去一个标记输出结果，维度为【批次，嵌入向量维度】\n",
    "        output = output.mean(axis=1)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "# 模型训练并保存\n",
    "def Transformer_model(train_loader, test_loader, config, nn_config):\n",
    "    # 选择训练硬件设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义Transformer网络\n",
    "    model = Transformer(nn_config.hidden_size, nn_config.num_layers, nn_config.feature_size, nn_config.output_size).to(device)\n",
    "    # 定义损失函数\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nn_config.learning_rate)\n",
    "    # 模型训练\n",
    "    train_loss_plot = []\n",
    "    test_loss_plot = []\n",
    "    for epoch in range(nn_config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        loss_plot = []\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_train_pred = model(x_train)\n",
    "            loss = loss_function(y_train_pred, y_train.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_plot.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, nn_config.epochs, loss)\n",
    "        train_loss_plot.append(min(loss_plot))\n",
    "    # 模型验证\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = loss_function(y_test_pred, y_test.reshape(-1, 1))\n",
    "            test_loss_plot.append(test_loss)\n",
    "    \n",
    "    save_path = './Model/{}.pth'.format(config.model_name)\n",
    "    if test_loss < nn_config.best_loss:\n",
    "        nn_config.modify('best_loss', test_loss)\n",
    "        torch.save(model, save_path)\n",
    "    else:\n",
    "        torch.save(model, save_path)\n",
    "    return train_loss_plot, test_loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/100] loss:163.837: 100%|██████████| 1753/1753 [00:08<00:00, 213.60it/s]  \n",
      "train epoch[2/100] loss:121.118: 100%|██████████| 1753/1753 [00:13<00:00, 131.45it/s]  \n",
      "train epoch[3/100] loss:90.620: 100%|██████████| 1753/1753 [00:07<00:00, 247.45it/s]   \n",
      "train epoch[4/100] loss:68.415: 100%|██████████| 1753/1753 [00:07<00:00, 244.28it/s]   \n",
      "train epoch[5/100] loss:51.925: 100%|██████████| 1753/1753 [00:07<00:00, 247.97it/s]   \n",
      "train epoch[6/100] loss:43.770: 100%|██████████| 1753/1753 [00:07<00:00, 233.92it/s]   \n",
      "train epoch[7/100] loss:42.708: 100%|██████████| 1753/1753 [00:07<00:00, 243.10it/s]   \n",
      "train epoch[8/100] loss:50.670: 100%|██████████| 1753/1753 [00:07<00:00, 242.13it/s]   \n",
      "train epoch[9/100] loss:68.298: 100%|██████████| 1753/1753 [00:07<00:00, 243.53it/s]   \n",
      "train epoch[10/100] loss:97.054: 100%|██████████| 1753/1753 [00:07<00:00, 239.52it/s]   \n",
      "train epoch[11/100] loss:137.360: 100%|██████████| 1753/1753 [00:07<00:00, 241.78it/s]  \n",
      "train epoch[12/100] loss:187.807: 100%|██████████| 1753/1753 [00:07<00:00, 243.35it/s]  \n",
      "train epoch[13/100] loss:252.170: 100%|██████████| 1753/1753 [00:07<00:00, 243.33it/s]  \n",
      "train epoch[14/100] loss:329.498: 100%|██████████| 1753/1753 [00:07<00:00, 242.25it/s]  \n",
      "train epoch[15/100] loss:416.971: 100%|██████████| 1753/1753 [00:07<00:00, 247.13it/s]  \n",
      "train epoch[16/100] loss:518.802: 100%|██████████| 1753/1753 [00:07<00:00, 240.04it/s]  \n",
      "train epoch[17/100] loss:627.366: 100%|██████████| 1753/1753 [00:07<00:00, 243.87it/s]  \n",
      "train epoch[18/100] loss:749.654: 100%|██████████| 1753/1753 [00:07<00:00, 244.21it/s]  \n",
      "train epoch[19/100] loss:882.935: 100%|██████████| 1753/1753 [00:07<00:00, 249.85it/s]  \n",
      "train epoch[20/100] loss:1010.101: 100%|██████████| 1753/1753 [00:07<00:00, 247.32it/s] \n",
      "train epoch[21/100] loss:1152.367: 100%|██████████| 1753/1753 [00:07<00:00, 237.48it/s] \n",
      "train epoch[22/100] loss:1291.856: 100%|██████████| 1753/1753 [00:07<00:00, 243.16it/s] \n",
      "train epoch[23/100] loss:1412.968: 100%|██████████| 1753/1753 [00:07<00:00, 246.35it/s] \n",
      "train epoch[24/100] loss:1503.576: 100%|██████████| 1753/1753 [00:07<00:00, 243.83it/s] \n",
      "train epoch[25/100] loss:1536.125: 100%|██████████| 1753/1753 [00:07<00:00, 241.12it/s] \n",
      "train epoch[26/100] loss:1481.911: 100%|██████████| 1753/1753 [00:07<00:00, 237.07it/s] \n",
      "train epoch[27/100] loss:1312.935: 100%|██████████| 1753/1753 [00:07<00:00, 246.93it/s] \n",
      "train epoch[28/100] loss:505.093: 100%|██████████| 1753/1753 [00:07<00:00, 242.78it/s]  \n",
      "train epoch[29/100] loss:157.850: 100%|██████████| 1753/1753 [00:07<00:00, 242.61it/s]  \n",
      "train epoch[30/100] loss:101.839: 100%|██████████| 1753/1753 [00:07<00:00, 238.50it/s]  \n",
      "train epoch[31/100] loss:65.055: 100%|██████████| 1753/1753 [00:12<00:00, 140.95it/s]   \n",
      "train epoch[32/100] loss:87.835: 100%|██████████| 1753/1753 [00:14<00:00, 123.64it/s]   \n",
      "train epoch[33/100] loss:53.046: 100%|██████████| 1753/1753 [00:14<00:00, 118.46it/s]   \n",
      "train epoch[34/100] loss:50.786: 100%|██████████| 1753/1753 [00:14<00:00, 123.85it/s]   \n",
      "train epoch[35/100] loss:63.802: 100%|██████████| 1753/1753 [00:14<00:00, 122.45it/s]   \n",
      "train epoch[36/100] loss:66.987: 100%|██████████| 1753/1753 [00:14<00:00, 120.57it/s]   \n",
      "train epoch[37/100] loss:114.788: 100%|██████████| 1753/1753 [00:14<00:00, 124.37it/s]  \n",
      "train epoch[38/100] loss:59.012: 100%|██████████| 1753/1753 [00:13<00:00, 125.27it/s]   \n",
      "train epoch[39/100] loss:55.630: 100%|██████████| 1753/1753 [00:13<00:00, 127.10it/s]   \n",
      "train epoch[40/100] loss:86.374: 100%|██████████| 1753/1753 [00:13<00:00, 127.42it/s]   \n",
      "train epoch[41/100] loss:75.676: 100%|██████████| 1753/1753 [00:13<00:00, 126.16it/s]   \n",
      "train epoch[42/100] loss:108.430: 100%|██████████| 1753/1753 [00:14<00:00, 123.63it/s]  \n",
      "train epoch[43/100] loss:104.245: 100%|██████████| 1753/1753 [00:14<00:00, 124.06it/s]  \n",
      "train epoch[44/100] loss:54.572: 100%|██████████| 1753/1753 [00:14<00:00, 121.16it/s]   \n",
      "train epoch[45/100] loss:68.076: 100%|██████████| 1753/1753 [00:14<00:00, 120.20it/s]   \n",
      "train epoch[46/100] loss:107.674: 100%|██████████| 1753/1753 [00:13<00:00, 125.55it/s]  \n",
      "train epoch[47/100] loss:106.564: 100%|██████████| 1753/1753 [00:13<00:00, 126.09it/s]  \n",
      "train epoch[48/100] loss:80.281: 100%|██████████| 1753/1753 [00:13<00:00, 126.99it/s]  \n",
      "train epoch[49/100] loss:60.485: 100%|██████████| 1753/1753 [00:13<00:00, 127.87it/s]  \n",
      "train epoch[50/100] loss:82.589: 100%|██████████| 1753/1753 [00:14<00:00, 124.07it/s]   \n",
      "train epoch[51/100] loss:151.009: 100%|██████████| 1753/1753 [00:14<00:00, 124.50it/s]  \n",
      "train epoch[52/100] loss:95.963: 100%|██████████| 1753/1753 [00:14<00:00, 122.33it/s]   \n",
      "train epoch[53/100] loss:83.404: 100%|██████████| 1753/1753 [00:14<00:00, 121.71it/s]   \n",
      "train epoch[54/100] loss:122.202: 100%|██████████| 1753/1753 [00:14<00:00, 123.04it/s]  \n",
      "train epoch[55/100] loss:45.176: 100%|██████████| 1753/1753 [00:14<00:00, 124.53it/s]  \n",
      "train epoch[56/100] loss:57.703: 100%|██████████| 1753/1753 [00:15<00:00, 116.36it/s]  \n",
      "train epoch[57/100] loss:90.193: 100%|██████████| 1753/1753 [00:13<00:00, 125.31it/s]  \n",
      "train epoch[58/100] loss:78.127: 100%|██████████| 1753/1753 [00:13<00:00, 126.06it/s]  \n",
      "train epoch[59/100] loss:89.937: 100%|██████████| 1753/1753 [00:14<00:00, 123.97it/s]  \n",
      "train epoch[60/100] loss:59.064: 100%|██████████| 1753/1753 [00:14<00:00, 123.38it/s]  \n",
      "train epoch[61/100] loss:97.233: 100%|██████████| 1753/1753 [00:14<00:00, 122.03it/s]   \n",
      "train epoch[62/100] loss:83.318: 100%|██████████| 1753/1753 [00:14<00:00, 118.85it/s]  \n",
      "train epoch[63/100] loss:66.444: 100%|██████████| 1753/1753 [00:13<00:00, 125.24it/s]  \n",
      "train epoch[64/100] loss:35.324: 100%|██████████| 1753/1753 [00:13<00:00, 126.33it/s]  \n",
      "train epoch[65/100] loss:110.019: 100%|██████████| 1753/1753 [00:13<00:00, 126.93it/s] \n",
      "train epoch[66/100] loss:60.543: 100%|██████████| 1753/1753 [00:13<00:00, 127.88it/s]  \n",
      "train epoch[67/100] loss:96.854: 100%|██████████| 1753/1753 [00:14<00:00, 124.67it/s]  \n",
      "train epoch[68/100] loss:61.151: 100%|██████████| 1753/1753 [00:14<00:00, 125.07it/s]  \n",
      "train epoch[69/100] loss:118.359: 100%|██████████| 1753/1753 [00:14<00:00, 121.41it/s] \n",
      "train epoch[70/100] loss:99.071: 100%|██████████| 1753/1753 [00:14<00:00, 119.88it/s]  \n",
      "train epoch[71/100] loss:168.901: 100%|██████████| 1753/1753 [00:14<00:00, 124.58it/s] \n",
      "train epoch[72/100] loss:81.939: 100%|██████████| 1753/1753 [00:13<00:00, 126.12it/s]  \n",
      "train epoch[73/100] loss:82.816: 100%|██████████| 1753/1753 [00:13<00:00, 126.22it/s]  \n",
      "train epoch[74/100] loss:37.140: 100%|██████████| 1753/1753 [00:13<00:00, 127.61it/s]  \n",
      "train epoch[75/100] loss:102.573: 100%|██████████| 1753/1753 [00:13<00:00, 126.00it/s] \n",
      "train epoch[76/100] loss:60.135: 100%|██████████| 1753/1753 [00:13<00:00, 128.91it/s]  \n",
      "train epoch[77/100] loss:51.211: 100%|██████████| 1753/1753 [00:13<00:00, 131.95it/s]  \n",
      "train epoch[78/100] loss:110.589: 100%|██████████| 1753/1753 [00:13<00:00, 130.38it/s] \n",
      "train epoch[79/100] loss:52.872: 100%|██████████| 1753/1753 [00:13<00:00, 131.18it/s]  \n",
      "train epoch[80/100] loss:98.713: 100%|██████████| 1753/1753 [00:13<00:00, 132.06it/s]  \n",
      "train epoch[81/100] loss:77.399: 100%|██████████| 1753/1753 [00:13<00:00, 132.10it/s]  \n",
      "train epoch[82/100] loss:57.049: 100%|██████████| 1753/1753 [00:13<00:00, 131.82it/s]  \n",
      "train epoch[83/100] loss:50.350: 100%|██████████| 1753/1753 [00:13<00:00, 132.53it/s]  \n",
      "train epoch[84/100] loss:91.101: 100%|██████████| 1753/1753 [00:13<00:00, 131.79it/s]  \n",
      "train epoch[85/100] loss:127.230: 100%|██████████| 1753/1753 [00:13<00:00, 132.15it/s] \n",
      "train epoch[86/100] loss:116.778: 100%|██████████| 1753/1753 [00:13<00:00, 131.37it/s] \n",
      "train epoch[87/100] loss:100.434: 100%|██████████| 1753/1753 [00:13<00:00, 131.33it/s] \n",
      "train epoch[88/100] loss:88.170: 100%|██████████| 1753/1753 [00:13<00:00, 131.81it/s]  \n",
      "train epoch[89/100] loss:63.503: 100%|██████████| 1753/1753 [00:13<00:00, 133.42it/s]  \n",
      "train epoch[90/100] loss:57.592: 100%|██████████| 1753/1753 [00:13<00:00, 133.35it/s]  \n",
      "train epoch[91/100] loss:101.418: 100%|██████████| 1753/1753 [00:13<00:00, 132.50it/s] \n",
      "train epoch[92/100] loss:57.914: 100%|██████████| 1753/1753 [00:13<00:00, 133.12it/s]  \n",
      "train epoch[93/100] loss:110.725: 100%|██████████| 1753/1753 [00:13<00:00, 132.30it/s] \n",
      "train epoch[94/100] loss:72.862: 100%|██████████| 1753/1753 [00:13<00:00, 131.92it/s]  \n",
      "train epoch[95/100] loss:59.885: 100%|██████████| 1753/1753 [00:15<00:00, 114.01it/s]  \n",
      "train epoch[96/100] loss:87.538: 100%|██████████| 1753/1753 [00:09<00:00, 185.70it/s]  \n",
      "train epoch[97/100] loss:97.649: 100%|██████████| 1753/1753 [00:06<00:00, 256.80it/s]  \n",
      "train epoch[98/100] loss:63.923: 100%|██████████| 1753/1753 [00:06<00:00, 253.22it/s]  \n",
      "train epoch[99/100] loss:71.580: 100%|██████████| 1753/1753 [00:06<00:00, 258.02it/s]  \n",
      "train epoch[100/100] loss:123.915: 100%|██████████| 1753/1753 [00:06<00:00, 259.58it/s]\n",
      "100%|██████████| 439/439 [00:00<00:00, 889.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE= 0.9236650965872573\n",
      "MAE= 11.5833845\n"
     ]
    }
   ],
   "source": [
    "# 模型调用过程\n",
    "# 1.初始化配置\n",
    "config = Config(train_size=0.8, timestep=12)\n",
    "# 2.初始化模型超参数\n",
    "nn_config = NN_config(batch_size=16, learning_rate=1e-6, epochs=100)\n",
    "# 3.根据使用模型修改名称\n",
    "config.modify_params('model_name', 'Transformer')\n",
    "# 4.读取数据\n",
    "data = read_data(config.data_path)\n",
    "# 5.划分训练集、测试集\n",
    "x_train, y_train, x_test, y_test = split_data_cnn(data, config.timestep, config.train_size)\n",
    "# 6.将数据格式转为tensor\n",
    "x_test_tensor, y_test_tensor, train_loader, test_loader = tensor_load_cnn(x_train, y_train, x_test, y_test, nn_config.batch_size)\n",
    "# 7.模型训练\n",
    "train_loss_plot, test_loss_plot = Transformer_model(train_loader, test_loader, config, nn_config)\n",
    "# 8.模型预测\n",
    "y_pre, NSE, MAE = prediction(x_test_tensor, y_test_tensor, config)\n",
    "# 9.结果保存\n",
    "save_results_cnn(y_test_tensor, y_pre, config)\n",
    "# 10.绘图展示（4.中展示）\n",
    "print(\"NSE=\", NSE)\n",
    "print(\"MAE=\", MAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建网络超参数类\n",
    "class NN_config:\n",
    "    def __init__(self, batch_size=16, feature_size=11, output_size=1, epochs=50, learning_rate=1e-8, best_loss=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.output_size = output_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_loss = best_loss\n",
    "        \n",
    "    def modify_params(self, param, value):\n",
    "        if param == 'batch_size':\n",
    "            self.batch_size = value\n",
    "        elif param == 'feature_size':\n",
    "            self.feature_size = value\n",
    "        elif param == 'output_size':\n",
    "            self.output_size = value\n",
    "        elif param == 'epochs':\n",
    "            self.epochs = value\n",
    "        elif param == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif param == 'best_loss':\n",
    "            self.best_loss = value\n",
    "        else:\n",
    "            raise SystemExit('The param is out the range OR value type is false')\n",
    "        \n",
    "# 定义Informer网络\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, feature_size, output_size, num_encoder_layers=2, num_decoder_layers=1, d_model=11, nhead=1, dim_feedforward=32, dropout=0.2, activation='relu'):\n",
    "        super(Informer, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.output_size = output_size\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        # 编码器\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_encoder_layers)\n",
    "        # 解码器\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=num_decoder_layers)\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # 编码器处理输入序列\n",
    "        enc_output = self.encoder(x)\n",
    "        # 解码器将编码器输出作为输入，并预测目标序列\n",
    "        dec_output = self.decoder(enc_output, enc_output)\n",
    "        # 取最后一个时间步长的输出，并通过全连接层得到最终输出\n",
    "        output = self.fc(dec_output[-1])\n",
    "        return output\n",
    "\n",
    "# 模型训练并保存\n",
    "def Informer_model(train_loader, test_loader, config, nn_config):\n",
    "    # 选择训练硬件设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 定义Insformer网络\n",
    "    model = Informer(nn_config.feature_size, nn_config.output_size).to(device)\n",
    "    # 定义损失函数\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nn_config.learning_rate)\n",
    "    # 模型训练\n",
    "    train_loss_plot = []\n",
    "    test_loss_plot = []\n",
    "    for epoch in range(nn_config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        loss_plot = []\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_train_pred = model(x_train)\n",
    "            loss = loss_function(y_train_pred, y_train.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_plot.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, nn_config.epochs, loss)\n",
    "    # 模型验证\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = loss_function(y_test_pred, y_test.reshape(-1, 1))\n",
    "            test_loss_plot.append(test_loss)\n",
    "            \n",
    "    save_path = './Model/{}.pth'.format(config.model_name)\n",
    "    if test_loss < nn_config.best_loss:\n",
    "        nn_config.modify('best_loss', test_loss)\n",
    "        torch.save(model, save_path)\n",
    "    else:\n",
    "        torch.save(model, save_path)\n",
    "    return train_loss_plot, test_loss_plot    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/100] loss:170.658: 100%|██████████| 1753/1753 [00:16<00:00, 103.62it/s]  \n",
      "train epoch[2/100] loss:164.643: 100%|██████████| 1753/1753 [00:11<00:00, 151.36it/s]  \n",
      "train epoch[3/100] loss:159.057: 100%|██████████| 1753/1753 [00:10<00:00, 161.31it/s]  \n",
      "train epoch[4/100] loss:153.558: 100%|██████████| 1753/1753 [00:10<00:00, 170.12it/s]  \n",
      "train epoch[5/100] loss:149.923: 100%|██████████| 1753/1753 [00:10<00:00, 173.97it/s]  \n",
      "train epoch[6/100] loss:143.567: 100%|██████████| 1753/1753 [00:09<00:00, 176.48it/s]  \n",
      "train epoch[7/100] loss:138.491: 100%|██████████| 1753/1753 [00:10<00:00, 169.26it/s]  \n",
      "train epoch[8/100] loss:133.067: 100%|██████████| 1753/1753 [00:10<00:00, 167.65it/s]  \n",
      "train epoch[9/100] loss:127.841: 100%|██████████| 1753/1753 [00:09<00:00, 181.02it/s]  \n",
      "train epoch[10/100] loss:122.818: 100%|██████████| 1753/1753 [00:10<00:00, 159.91it/s]  \n",
      "train epoch[11/100] loss:117.403: 100%|██████████| 1753/1753 [00:10<00:00, 166.90it/s]  \n",
      "train epoch[12/100] loss:112.383: 100%|██████████| 1753/1753 [00:11<00:00, 154.30it/s]  \n",
      "train epoch[13/100] loss:106.890: 100%|██████████| 1753/1753 [00:11<00:00, 158.18it/s]  \n",
      "train epoch[14/100] loss:102.533: 100%|██████████| 1753/1753 [00:10<00:00, 167.59it/s]  \n",
      "train epoch[15/100] loss:97.109: 100%|██████████| 1753/1753 [00:10<00:00, 161.22it/s]   \n",
      "train epoch[16/100] loss:92.164: 100%|██████████| 1753/1753 [00:10<00:00, 161.72it/s]   \n",
      "train epoch[17/100] loss:87.585: 100%|██████████| 1753/1753 [00:11<00:00, 157.78it/s]   \n",
      "train epoch[18/100] loss:82.915: 100%|██████████| 1753/1753 [00:10<00:00, 163.92it/s]   \n",
      "train epoch[19/100] loss:78.564: 100%|██████████| 1753/1753 [00:10<00:00, 165.35it/s]   \n",
      "train epoch[20/100] loss:74.315: 100%|██████████| 1753/1753 [00:11<00:00, 155.54it/s]   \n",
      "train epoch[21/100] loss:70.174: 100%|██████████| 1753/1753 [00:10<00:00, 167.56it/s]   \n",
      "train epoch[22/100] loss:66.453: 100%|██████████| 1753/1753 [00:10<00:00, 167.63it/s]   \n",
      "train epoch[23/100] loss:62.774: 100%|██████████| 1753/1753 [00:11<00:00, 157.20it/s]   \n",
      "train epoch[24/100] loss:59.286: 100%|██████████| 1753/1753 [00:10<00:00, 166.34it/s]   \n",
      "train epoch[25/100] loss:56.154: 100%|██████████| 1753/1753 [00:10<00:00, 161.03it/s]   \n",
      "train epoch[26/100] loss:53.305: 100%|██████████| 1753/1753 [00:10<00:00, 165.09it/s]   \n",
      "train epoch[27/100] loss:50.764: 100%|██████████| 1753/1753 [00:10<00:00, 161.94it/s]   \n",
      "train epoch[28/100] loss:48.385: 100%|██████████| 1753/1753 [00:10<00:00, 162.12it/s]   \n",
      "train epoch[29/100] loss:46.265: 100%|██████████| 1753/1753 [00:10<00:00, 165.21it/s]   \n",
      "train epoch[30/100] loss:44.767: 100%|██████████| 1753/1753 [00:11<00:00, 158.24it/s]   \n",
      "train epoch[31/100] loss:43.451: 100%|██████████| 1753/1753 [00:10<00:00, 162.11it/s]   \n",
      "train epoch[32/100] loss:42.624: 100%|██████████| 1753/1753 [00:10<00:00, 172.35it/s]   \n",
      "train epoch[33/100] loss:42.176: 100%|██████████| 1753/1753 [00:10<00:00, 174.71it/s]   \n",
      "train epoch[34/100] loss:42.064: 100%|██████████| 1753/1753 [00:09<00:00, 186.22it/s]   \n",
      "train epoch[35/100] loss:42.480: 100%|██████████| 1753/1753 [00:09<00:00, 186.42it/s]   \n",
      "train epoch[36/100] loss:43.085: 100%|██████████| 1753/1753 [00:10<00:00, 168.45it/s]   \n",
      "train epoch[37/100] loss:44.490: 100%|██████████| 1753/1753 [00:09<00:00, 187.64it/s]   \n",
      "train epoch[38/100] loss:46.152: 100%|██████████| 1753/1753 [00:09<00:00, 179.35it/s]   \n",
      "train epoch[39/100] loss:48.394: 100%|██████████| 1753/1753 [00:09<00:00, 175.92it/s]   \n",
      "train epoch[40/100] loss:51.122: 100%|██████████| 1753/1753 [00:09<00:00, 177.89it/s]   \n",
      "train epoch[41/100] loss:54.339: 100%|██████████| 1753/1753 [00:10<00:00, 173.04it/s]   \n",
      "train epoch[42/100] loss:57.781: 100%|██████████| 1753/1753 [00:09<00:00, 189.58it/s]   \n",
      "train epoch[43/100] loss:62.275: 100%|██████████| 1753/1753 [00:10<00:00, 172.06it/s]   \n",
      "train epoch[44/100] loss:66.776: 100%|██████████| 1753/1753 [00:10<00:00, 173.65it/s]   \n",
      "train epoch[45/100] loss:71.450: 100%|██████████| 1753/1753 [00:09<00:00, 178.19it/s]   \n",
      "train epoch[46/100] loss:76.631: 100%|██████████| 1753/1753 [00:09<00:00, 176.51it/s]   \n",
      "train epoch[47/100] loss:81.003: 100%|██████████| 1753/1753 [00:09<00:00, 179.41it/s]   \n",
      "train epoch[48/100] loss:84.732: 100%|██████████| 1753/1753 [00:10<00:00, 174.00it/s]   \n",
      "train epoch[49/100] loss:88.962: 100%|██████████| 1753/1753 [00:09<00:00, 177.15it/s]   \n",
      "train epoch[50/100] loss:86.567: 100%|██████████| 1753/1753 [00:10<00:00, 172.52it/s]   \n",
      "train epoch[51/100] loss:83.875: 100%|██████████| 1753/1753 [00:10<00:00, 168.14it/s]   \n",
      "train epoch[52/100] loss:84.326: 100%|██████████| 1753/1753 [00:10<00:00, 163.64it/s]   \n",
      "train epoch[53/100] loss:89.337: 100%|██████████| 1753/1753 [00:10<00:00, 167.81it/s]   \n",
      "train epoch[54/100] loss:79.104: 100%|██████████| 1753/1753 [00:10<00:00, 167.12it/s]   \n",
      "train epoch[55/100] loss:95.914: 100%|██████████| 1753/1753 [00:10<00:00, 162.65it/s]   \n",
      "train epoch[56/100] loss:68.829: 100%|██████████| 1753/1753 [00:10<00:00, 171.16it/s]   \n",
      "train epoch[57/100] loss:83.104: 100%|██████████| 1753/1753 [00:10<00:00, 163.14it/s]   \n",
      "train epoch[58/100] loss:65.750: 100%|██████████| 1753/1753 [00:10<00:00, 169.82it/s]   \n",
      "train epoch[59/100] loss:86.086: 100%|██████████| 1753/1753 [00:10<00:00, 167.84it/s]   \n",
      "train epoch[60/100] loss:97.870: 100%|██████████| 1753/1753 [00:10<00:00, 165.04it/s]   \n",
      "train epoch[61/100] loss:75.050: 100%|██████████| 1753/1753 [00:10<00:00, 163.74it/s]   \n",
      "train epoch[62/100] loss:70.254: 100%|██████████| 1753/1753 [00:10<00:00, 167.09it/s]   \n",
      "train epoch[63/100] loss:83.882: 100%|██████████| 1753/1753 [00:10<00:00, 170.99it/s]   \n",
      "train epoch[64/100] loss:81.723: 100%|██████████| 1753/1753 [00:10<00:00, 169.58it/s]   \n",
      "train epoch[65/100] loss:63.420: 100%|██████████| 1753/1753 [00:10<00:00, 169.61it/s]   \n",
      "train epoch[66/100] loss:95.841: 100%|██████████| 1753/1753 [00:10<00:00, 168.78it/s]   \n",
      "train epoch[67/100] loss:78.791: 100%|██████████| 1753/1753 [00:10<00:00, 171.98it/s]   \n",
      "train epoch[68/100] loss:54.169: 100%|██████████| 1753/1753 [00:10<00:00, 174.76it/s]   \n",
      "train epoch[69/100] loss:97.015: 100%|██████████| 1753/1753 [00:09<00:00, 175.54it/s]   \n",
      "train epoch[70/100] loss:69.777: 100%|██████████| 1753/1753 [00:09<00:00, 176.34it/s]   \n",
      "train epoch[71/100] loss:66.149: 100%|██████████| 1753/1753 [00:09<00:00, 179.23it/s]   \n",
      "train epoch[72/100] loss:72.641: 100%|██████████| 1753/1753 [00:09<00:00, 182.96it/s]   \n",
      "train epoch[73/100] loss:76.451: 100%|██████████| 1753/1753 [00:10<00:00, 169.21it/s]   \n",
      "train epoch[74/100] loss:115.712: 100%|██████████| 1753/1753 [00:09<00:00, 184.55it/s]  \n",
      "train epoch[75/100] loss:66.969: 100%|██████████| 1753/1753 [00:09<00:00, 181.19it/s]   \n",
      "train epoch[76/100] loss:68.166: 100%|██████████| 1753/1753 [00:10<00:00, 174.75it/s]   \n",
      "train epoch[77/100] loss:91.594: 100%|██████████| 1753/1753 [00:10<00:00, 174.70it/s]   \n",
      "train epoch[78/100] loss:85.817: 100%|██████████| 1753/1753 [00:09<00:00, 187.17it/s]   \n",
      "train epoch[79/100] loss:86.719: 100%|██████████| 1753/1753 [00:09<00:00, 178.98it/s]   \n",
      "train epoch[80/100] loss:109.074: 100%|██████████| 1753/1753 [00:09<00:00, 179.05it/s]  \n",
      "train epoch[81/100] loss:97.655: 100%|██████████| 1753/1753 [00:09<00:00, 177.81it/s]   \n",
      "train epoch[82/100] loss:89.604: 100%|██████████| 1753/1753 [00:10<00:00, 175.24it/s]   \n",
      "train epoch[83/100] loss:85.444: 100%|██████████| 1753/1753 [00:09<00:00, 179.35it/s]   \n",
      "train epoch[84/100] loss:45.815: 100%|██████████| 1753/1753 [00:09<00:00, 175.77it/s]   \n",
      "train epoch[85/100] loss:89.354: 100%|██████████| 1753/1753 [00:09<00:00, 178.14it/s]   \n",
      "train epoch[86/100] loss:51.453: 100%|██████████| 1753/1753 [00:10<00:00, 168.11it/s]   \n",
      "train epoch[87/100] loss:76.157: 100%|██████████| 1753/1753 [00:10<00:00, 162.09it/s]   \n",
      "train epoch[88/100] loss:56.216: 100%|██████████| 1753/1753 [00:10<00:00, 167.23it/s]   \n",
      "train epoch[89/100] loss:68.475: 100%|██████████| 1753/1753 [00:10<00:00, 164.37it/s]   \n",
      "train epoch[90/100] loss:91.259: 100%|██████████| 1753/1753 [00:10<00:00, 163.73it/s]   \n",
      "train epoch[91/100] loss:42.038: 100%|██████████| 1753/1753 [00:10<00:00, 165.76it/s]   \n",
      "train epoch[92/100] loss:65.869: 100%|██████████| 1753/1753 [00:10<00:00, 165.80it/s]   \n",
      "train epoch[93/100] loss:126.716: 100%|██████████| 1753/1753 [00:10<00:00, 166.42it/s]  \n",
      "train epoch[94/100] loss:140.270: 100%|██████████| 1753/1753 [00:10<00:00, 167.82it/s]  \n",
      "train epoch[95/100] loss:55.600: 100%|██████████| 1753/1753 [00:10<00:00, 164.91it/s]   \n",
      "train epoch[96/100] loss:66.035: 100%|██████████| 1753/1753 [00:10<00:00, 168.38it/s]   \n",
      "train epoch[97/100] loss:90.074: 100%|██████████| 1753/1753 [00:10<00:00, 167.00it/s]   \n",
      "train epoch[98/100] loss:44.333: 100%|██████████| 1753/1753 [00:10<00:00, 165.41it/s]   \n",
      "train epoch[99/100] loss:65.141: 100%|██████████| 1753/1753 [00:10<00:00, 168.74it/s]   \n",
      "train epoch[100/100] loss:119.609: 100%|██████████| 1753/1753 [00:10<00:00, 169.65it/s]  \n",
      "100%|██████████| 439/439 [00:00<00:00, 631.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE= 0.032096411021306004\n",
      "MAE= 38.674965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型调用过程\n",
    "# 1.初始化配置\n",
    "config = Config(train_size=0.8, timestep=12)\n",
    "# 2.初始化模型超参数\n",
    "nn_config = NN_config(batch_size=16, learning_rate=1e-5, epochs=100)\n",
    "# 3.根据使用模型修改名称\n",
    "config.modify_params('model_name', 'Informer')\n",
    "# 4.读取数据\n",
    "data = read_data(config.data_path)\n",
    "# 5.划分训练集、测试集\n",
    "x_train, y_train, x_test, y_test = split_data_cnn(data, config.timestep, config.train_size)\n",
    "# 6.将数据格式转为tensor\n",
    "x_test_tensor, y_test_tensor, train_loader, test_loader = tensor_load_cnn(x_train, y_train, x_test, y_test, nn_config.batch_size)\n",
    "# 7.模型训练\n",
    "train_loss_plot, test_loss_plot = Informer_model(train_loader, test_loader, config, nn_config)\n",
    "# 8.模型预测\n",
    "y_pre, NSE, MAE = prediction(x_test_tensor, y_test_tensor, config)\n",
    "# 9.结果保存\n",
    "save_results_cnn(y_test_tensor, y_pre, config)\n",
    "# 10.绘图展示（4.中展示）\n",
    "print(\"NSE=\", NSE)\n",
    "print(\"MAE=\", MAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
